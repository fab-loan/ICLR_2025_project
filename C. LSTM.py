#TASK C - RNN & LSTM
# -*- coding: utf-8 -*-
"""QBUS6860 Task C - 25 May.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RbPcUZbzkCHVLWKbWkbK-bb3Otqdx8Oj
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
import re
!pip install keras-tuner -q

#pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118

#pip install torchtext

import torch
import torch.nn as nn
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torchtext.vocab import vocab
from collections import Counter
torch.manual_seed(0)

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.util import ngrams
from sklearn.feature_extraction.text import TfidfVectorizer

# Setup
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')
lemmatizer = WordNetLemmatizer()

# Initialize tools
tokenizer = get_tokenizer("basic_english")
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

"""# Read Files + Impute Missing Values"""

df_sub = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Submissions.csv')
df_sub.head(5)

df_test_original = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Rating_Test.csv')
df_test_original.head(5)

df_train_original = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Rating_Train.csv')
df_train_original .head(5)

"""Missing Values


*   Found in 'stengths' & 'weakness'
*   Impute with a Placeholder --> Since it's text data, you can replace missing values with a placeholder string such as 'No data available' or 'Not provided'. This way, you can retain all rows without losing any data.

Duplicate Values

* One paper is check multiple times, it has different scores and different strenghts & weaknesses

"""

# Check for missing values in the training, validation, and test data
print("Missing values in submissions data:")
print(df_sub.isnull().sum())

# Check basic information about the datasets (e.g., data types, number of non-null entries)
print("\nSubmissions data info:")
print(df_sub.info())

# Check for duplicate rows (excluding ID columns)
print("\nDuplicate rows in training data (excluding review_ID_train):",
      df_sub.duplicated(subset=[col for col in df_sub.columns if col != 'reviewID_train']).sum())

# Summary statistics to understand the distribution of numerical columns
print("\nSubmissions data summary statistics:")
print(df_sub.describe())

# Drop TLDR column
df_sub.drop(columns=['tldr'], inplace=True)

# Check for missing values in the training, validation, and test data
print("Missing values in training data:")
print(df_train_original.isnull().sum())

# Check basic information about the datasets (e.g., data types, number of non-null entries)
print("\nTraining data info:")
print(df_train_original.info())

# Check for duplicate rows (excluding ID columns)
print("\nDuplicate rows in training data (excluding review_ID_train):",
      df_train_original.duplicated(subset=[col for col in df_train_original.columns if col != 'reviewID_train']).sum())

# Summary statistics to understand the distribution of numerical columns
print("\nTraining data summary statistics:")
print(df_train_original.describe())

# Impute missing values with 'Not provided' for Strengths and Weaknesses and Summary in ICLR_train, ICLR_vali, and ICLR_test
df_train_original['Strengths'] = df_train_original['Strengths'].fillna('Not provided')
df_train_original['Weaknesses'] = df_train_original['Weaknesses'].fillna('Not provided')
df_train_original['Summary'] = df_train_original['Summary'].fillna('Not provided')

# Check whether missing values are gone
print("Missing values in training data:")
print(df_train_original[['Strengths', 'Weaknesses','Summary']].isnull().sum())

"""# Text Pre-Processing

Run the Training data Pre Processed CSV file instead of the Text Pre-Processing section - for faster pre-processing

1. Text Cleaning

* convert to lowercase
* remove URLs
* remove punctuation, other characters
* remove digits/ numbers
"""

# Define your preprocessing function

# Precompile URL regex
url_pattern = re.compile(r'https?://\S+|www\.\S+')

def clean_text(text):
    text = text.lower() # Convert to lowercase
    text = re.sub(r'[^a-z\s]', ' ', text)  # Remove numbers, punctuation, special chars
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    return text

# Columns to process
text_columns = ['title', 'abstract', 'Summary', 'Strengths', 'Weaknesses']

# Apply preprocessing directly (since no NaNs exist)
for col in ['title', 'abstract', 'Summary', 'Strengths', 'Weaknesses']:
    df_train_original[col] = df_train_original[col].apply(clean_text)

"""2. Tokenization"""

# Initialize basic English tokenizer
tokenizer = get_tokenizer("basic_english")

# Tokenize each text column
for col in ['title', 'abstract', 'Summary', 'Strengths', 'Weaknesses']:
    token_col = col + '_tokens'
    df_train_original[token_col] = df_train_original[col].apply(tokenizer)

"""3. Stop-Word Removal --> NLTK"""

# Load English stopwords
stop_words = set(stopwords.words('english'))

# Remove stopwords from tokenized columns
for col in ['title_tokens', 'abstract_tokens', 'Summary_tokens', 'Strengths_tokens', 'Weaknesses_tokens']:
    df_train_original[col] = df_train_original[col].apply(lambda tokens: [token for token in tokens if token not in stop_words])

"""4. Lemmatization --> WordNetLemmatizer & POS_tag"""

#import nltk
from nltk.corpus import wordnet
from nltk import pos_tag

# Download required NLTK resources
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to convert POS tag to WordNet format
def get_wordnet_pos(word):
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {
        'J': wordnet.ADJ,
        'N': wordnet.NOUN,
        'V': wordnet.VERB,
        'R': wordnet.ADV
    }
    return tag_dict.get(tag, wordnet.NOUN)

# Function to lemmatize a list of tokens using POS
def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]

for col in ['title_tokens', 'abstract_tokens', 'Summary_tokens', 'Strengths_tokens', 'Weaknesses_tokens']:
    df_train_original[col] = df_train_original[col].apply(lemmatize_tokens)

df_train_original.head(2)

"""saving pre-processed df_train_original to a csv"""

df_train_original.to_csv("df_train_original_processed.csv", index=False)

"""# Training data Pre-processed CSV file"""

df_train_processed= pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/df_train_original_processed.csv')

df_train_processed_without = df_train_processed.drop(columns=['title', 'abstract','title_tokens','abstract_tokens'])
df_train_processed_without.head(2)

"""### combining tokens"""

# Combine tokens into one list per row
df_train_processed['combined_tokens'] = df_train_processed['title_tokens'] + df_train_processed['abstract_tokens'] + df_train_processed['Summary_tokens'] + df_train_processed['Strengths_tokens'] + df_train_processed['Weaknesses_tokens']

df_train_processed.head(5)

df_train_processed_without['combined_tokens_without'] = df_train_processed['Summary_tokens'] + df_train_processed['Strengths_tokens'] + df_train_processed['Weaknesses_tokens']

df_train_processed_without.head(2)

"""# Train-Vali-Split

stratify split to account for imbalanced classes
"""

# Step 1: 70% train, 30% temp (stratified)
df_train, df_temp = train_test_split(
    df_train_processed,
    test_size=0.30,
    stratify=df_train_processed['Rating'],
    random_state=42
)

# Step 2: Split temp into 20% validation and 10% test
# Use stratify again to preserve distribution
df_vali, df_test = train_test_split(
    df_temp,
    test_size=0.33,  # 33% of 30% ‚âà 10% of total
    stratify=df_temp['Rating'],
    random_state=42
)

print("Train shape:", df_train.shape)
print("Validation shape:", df_vali.shape)
print("Test shape:", df_test.shape)

"""# Feature Engineering

Scalling features such as Soundness, Presentation and Contribution 1/4, while confidence is out of 5.

**Scalling with StandardScaler should only be applied to the training set to avoid data leakage**

There is no need to scale the target variable (Y), even if it‚Äôs on a different scale from the input features.




---



Use Min-Max Scaling to Normalize All Ratings to the [0, 1] Range
Why?

LSTM models (and neural networks in general) perform best when inputs are on a consistent scale.

Min-Max scaling is ideal for ordinal ratings (like 1‚Äì4 or 1‚Äì5) because it:


*   Preserves the relative order
*   Keeps values in a bounded [0, 1] range, improving training stability and convergence
"""

# Step 3: Feature Scaling (X)
# ----------------------------
from sklearn.preprocessing import MinMaxScaler

# Columns to scale
numcl = ['Soundness', 'Presentation', 'Contribution', 'Confidence']

# Initialize MinMaxScaler (default scales to [0, 1])
scaler = MinMaxScaler()

# Fit scaler on training data only
df_train_scaled = df_train.copy()
df_train_scaled[numcl] = scaler.fit_transform(df_train[numcl])

# Transform validation and test sets with the same scaler
df_vali_scaled = df_vali.copy()
df_vali_scaled[numcl] = scaler.transform(df_vali[numcl])

df_test_scaled = df_test.copy()
df_test_scaled[numcl] = scaler.transform(df_test[numcl])

# Rating  0 ~ num_classes-1
all_labels = sorted(df_train_processed['Rating'].unique())
rating_map = {v: i for i, v in enumerate(all_labels)}

for df in [df_train, df_vali, df_test]:
    df['Rating'] = df['Rating'].map(rating_map)

df_train.head(2)

df_train_scaled.head(2)

"""# Defining RNN and LSTM Models"""

textcol = ['combined_tokens']
numcl = ['Soundness','Presentation','Contribution','Confidence']
y = ['Rating']

# # Rating  0 ~ num_classes-1
# all_labels = sorted(df_train_processed['Rating'].unique())
# rating_map = {v: i for i, v in enumerate(all_labels)}

# for df in [df_train, df_vali, df_test]:
#     df['Rating'] = df['Rating'].map(rating_map)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Fit tokenizer on the full text corpus
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df_train['combined_tokens'])

# Convert texts to sequences
train_seq = tokenizer.texts_to_sequences(df_train['combined_tokens'])
val_seq = tokenizer.texts_to_sequences(df_vali['combined_tokens'])
test_seq = tokenizer.texts_to_sequences(df_test['combined_tokens'])

# Pad sequences
X_text_train = pad_sequences(train_seq, padding='post', maxlen=300)
X_text_val   = pad_sequences(val_seq, padding='post', maxlen=300)
X_text_test  = pad_sequences(test_seq, padding='post', maxlen=300)

# Use scaled versions for numeric input features
X_num_train = df_train_scaled[numcl].values.astype(np.float32)
X_num_val   = df_vali_scaled[numcl].values.astype(np.float32)
X_num_test  = df_test_scaled[numcl].values.astype(np.float32)

# Use the same (scaled) DataFrames for labels if Rating mapping was applied to them
y_train = df_train_scaled['Rating'].values.astype(np.int32)
y_val   = df_vali_scaled['Rating'].values.astype(np.int32)
y_test  = df_test_scaled['Rating'].values.astype(np.int32)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_val = le.transform(y_val)
y_test = le.transform(y_test)

print("Max token index in X_text_train:", np.max(X_text_train))

"""## RNN Base Model & Hyperparameter Tuning"""

import shutil
shutil.rmtree('rnn_tuning', ignore_errors=True)

!pip install keras-tuner -q

# import shutil
import random
import tensorflow as tf
from keras_tuner import HyperModel, RandomSearch
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping


# shutil.rmtree('rnn_tuning', ignore_errors=True)

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

EMBED_DIM_RANGE = (448, 640, 64)
RNN_HIDDEN_RANGE = (32, 128, 32)
DENSE_UNITS_RANGE = (32, 128, 32)
DROPOUT_RANGE = (0.3, 0.7, 0.1)
LEARNING_RATE_OPTIONS = [1e-3, 1e-4]


class TextNumericHyperModel(HyperModel):
    def __init__(self, vocab_size, input_text_len, num_numeric_features, num_classes):
        self.vocab_size = vocab_size
        self.input_text_len = input_text_len
        self.num_numeric_features = num_numeric_features
        self.num_classes = num_classes

    def build(self, hp):
        embed_dim = hp.Int('embed_dim', *EMBED_DIM_RANGE)
        rnn_hidden = hp.Int('rnn_hidden', *RNN_HIDDEN_RANGE)
        dense_units = hp.Int('dense_units', *DENSE_UNITS_RANGE)
        dropout_rate = hp.Float('dropout', *DROPOUT_RANGE)
        learning_rate = hp.Choice('lr', LEARNING_RATE_OPTIONS)

        text_input = Input(shape=(self.input_text_len,), name='text_input')
        x = Embedding(input_dim=self.vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
        x = SimpleRNN(rnn_hidden)(x)

        num_input = Input(shape=(self.num_numeric_features,), name='num_input')
        concat = Concatenate()([x, num_input])
        x = Dense(dense_units)(concat)
        x = ReLU()(x)
        x = Dropout(dropout_rate)(x)
        output = Dense(self.num_classes, activation='softmax')(x)

        model = Model(inputs=[text_input, num_input], outputs=output)
        model.compile(
            optimizer=Adam(learning_rate),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model


hypermodel = TextNumericHyperModel(
    vocab_size=np.max(X_text_train) + 1,
    input_text_len=X_text_train.shape[1],
    num_numeric_features=X_num_train.shape[1],
    num_classes=len(rating_map)
)


tuner = RandomSearch(
    hypermodel,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    seed=SEED,
    directory='rnn_tuning',
    project_name='text_num_rnn'
)


tuner.search(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=3,
    batch_size=32,
    callbacks=[EarlyStopping(monitor='val_loss', patience=2)]
)


best_model = tuner.get_best_models(num_models=1)[0]


best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters:")
print(f"Embedding dimension: {best_hps.get('embed_dim')}")
print(f"RNN hidden units: {best_hps.get('rnn_hidden')}")
print(f"Dense units: {best_hps.get('dense_units')}")
print(f"Dropout rate: {best_hps.get('dropout')}")
print(f"Learning rate: {best_hps.get('lr')}")

checkpoint_path = "best_model.weights.h5"



callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, verbose=1),
    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)
]


history = best_model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=10,
    batch_size=32,
    callbacks=callbacks
)


best_model.load_weights(checkpoint_path)


y_pred_probs = best_model.predict([X_text_val, X_num_val])
y_pred = np.argmax(y_pred_probs, axis=1)

print(classification_report(y_val, y_pred))
weighted_f1 = f1_score(y_val, y_pred, average='weighted')
print(f"Weighted F1-score (best epoch): {weighted_f1:.4f}")

"""## LSTM with hyperparameter tuning without weights, with embedded layers"""

!pip install keras-tuner -q

from keras_tuner import HyperModel, RandomSearch
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np


EMBED_DIM_RANGE = (448, 640, 64)         # min, max, step
LSTM_HIDDEN_RANGE = (32, 128, 32)
DENSE_UNITS_RANGE = (32, 128, 32)
DROPOUT_RANGE = (0.3, 0.7, 0.1)
LEARNING_RATE_OPTIONS = [1e-3, 1e-4]


class TextNumericHyperModel(HyperModel):
    def __init__(self, vocab_size, input_text_len, num_numeric_features, num_classes):
        self.vocab_size = vocab_size
        self.input_text_len = input_text_len
        self.num_numeric_features = num_numeric_features
        self.num_classes = num_classes

    def build(self, hp):
        embed_dim = hp.Int('embed_dim', *EMBED_DIM_RANGE)
        lstm_hidden = hp.Int('lstm_hidden', *LSTM_HIDDEN_RANGE)
        dense_units = hp.Int('dense_units', *DENSE_UNITS_RANGE)
        dropout_rate = hp.Float('dropout', *DROPOUT_RANGE)
        learning_rate = hp.Choice('lr', LEARNING_RATE_OPTIONS)

        text_input = Input(shape=(self.input_text_len,), name='text_input')
        x = Embedding(input_dim=self.vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
        x = LSTM(lstm_hidden)(x)

        num_input = Input(shape=(self.num_numeric_features,), name='num_input')
        concat = Concatenate()([x, num_input])
        x = Dense(dense_units)(concat)
        x = ReLU()(x)
        x = Dropout(dropout_rate)(x)
        output = Dense(self.num_classes, activation='softmax')(x)

        model = Model(inputs=[text_input, num_input], outputs=output)
        model.compile(
            optimizer=Adam(learning_rate),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model


hypermodel = TextNumericHyperModel(
    vocab_size=np.max(X_text_train) + 1,
    input_text_len=X_text_train.shape[1],
    num_numeric_features=X_num_train.shape[1],
    num_classes=len(rating_map)
)

tuner = RandomSearch(
    hypermodel,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='rnn_tuning',
    project_name='text_num_model'
)

tuner.search(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=3,
    batch_size=32,
    callbacks=[EarlyStopping(monitor='val_loss', patience=2)]
)
best_model = tuner.get_best_models(num_models=1)[0]

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

print("Best Hyperparameters Found:")
print(f"Embedding dimension: {best_hps.get('embed_dim')}")
print(f"LSTM hidden units: {best_hps.get('lstm_hidden')}")
print(f"Dense units: {best_hps.get('dense_units')}")
print(f"Dropout rate: {best_hps.get('dropout')}")
print(f"Learning rate: {best_hps.get('lr')}")

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import classification_report, f1_score
import numpy as np
import os


checkpoint_path = "best_model.weights.h5"



callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, verbose=1),
    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)
]


history = best_model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=10,
    batch_size=32,
    callbacks=callbacks
)


best_model.load_weights(checkpoint_path)


y_pred_probs = best_model.predict([X_text_val, X_num_val])
y_pred = np.argmax(y_pred_probs, axis=1)

print(classification_report(y_val, y_pred))
weighted_f1 = f1_score(y_val, y_pred, average='weighted')
print(f"Weighted F1-score (best epoch): {weighted_f1:.4f}")

"""## 1

"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam


vocab_size = np.max(X_text_train) + 1
embed_dim = 584
lstm_hidden = 64
num_classes = len(rating_map)
num_numeric_features = X_num_train.shape[1]


text_input = Input(shape=(X_text_train.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden)(x)


num_input = Input(shape=(num_numeric_features,), name='num_input')


concat = Concatenate()([x, num_input])
x = Dense(64)(concat)
x = ReLU()(x)
x = Dropout(0.3)(x)
output = Dense(num_classes, activation='softmax')(x)


model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

from sklearn.utils.class_weight import compute_class_weight
import numpy as np
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_accuracy',
    patience=2,
    restore_best_weights=True
)

class_weights = compute_class_weight(
    class_weight='unbalanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = dict(enumerate(class_weights))


history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=5,
    batch_size=32,
    callbacks=[early_stop],
    class_weight=class_weight_dict
)

from sklearn.metrics import f1_score
import numpy as np


y_val_pred_probs = model.predict([X_text_val, X_num_val], verbose=0)
y_val_pred = np.argmax(y_val_pred_probs, axis=1)

# ÊâìÂç∞ weighted average F1-score
f1 = f1_score(y_val, y_val_pred, average='weighted')
print(f"Weighted F1-score on validation set: {f1:.4f}")

from sklearn.metrics import classification_report
print(classification_report(y_val, y_val_pred, digits=4))



"""## one layer LSTM (with embedded layers without weights) -- MODEL A
title + abstract + summary + soundness + presentation + contribution + strengths + weaknesses + confidence

# MODEL A - refined code with early stopping, training and visualisation
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import f1_score, classification_report

# --------------------------
# 1. Model Configuration
# --------------------------

# Define dimensions
vocab_size = np.max(X_text_train) + 1  # assuming X_text_train contains integer-encoded tokens
embed_dim = 584
lstm_hidden = 64
num_classes = len(np.unique(y_train))  # or len(rating_map)
num_numeric_features = X_num_train.shape[1]

# --------------------------
# 2. Model Architecture
# --------------------------

# Text input
text_input = Input(shape=(X_text_train.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden)(x)

# Numeric input
num_input = Input(shape=(num_numeric_features,), name='num_input')

# Concatenate text + numeric features
concat = Concatenate()([x, num_input])
x = Dense(64)(concat)
x = ReLU()(x)
x = Dropout(0.5)(x)
output = Dense(num_classes, activation='softmax')(x)

# Build and compile model
model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# --------------------------
# 3. Training
# --------------------------

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

# --------------------------
# 4. Evaluation
# --------------------------

# Predict on validation set
y_val_pred = model.predict([X_text_val, X_num_val]).argmax(axis=1)

# Macro F1 score (treats all classes equally)
f1 = f1_score(y_val, y_val_pred, average='macro')
print(f"\n Macro F1 Score: {f1:.4f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_val, y_val_pred))

# --------------------------
# 5. Visualization
# --------------------------

plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""reduced learning rate and emb dimensions to 512"""

import tensorflow.keras.backend as K
from tensorflow.keras.layers import Layer

# ----------- Mask-Supporting Attention Layer -----------
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(shape=(input_shape[-1], 1),
                                 initializer='glorot_uniform',
                                 trainable=True)
        self.b = self.add_weight(shape=(1,),
                                 initializer='zeros',
                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs, mask=None):
        score = K.tanh(K.dot(inputs, self.W) + self.b)  # (batch_size, seq_len, 1)
        score = K.squeeze(score, axis=-1)               # (batch_size, seq_len)

        # Apply the mask: set padded tokens to a large negative value
        if mask is not None:
            score -= 1e9 * (1.0 - K.cast(mask, K.floatx()))

        attention_weights = K.softmax(score, axis=1)              # (batch_size, seq_len)
        attention_weights = K.expand_dims(attention_weights, -1)  # (batch_size, seq_len, 1)

        context_vector = attention_weights * inputs               # (batch_size, seq_len, units)
        context_vector = K.sum(context_vector, axis=1)            # (batch_size, units)

        return context_vector

    def compute_mask(self, inputs, mask=None):
        # Output is a single context vector, so no downstream mask is needed
        return None

# ----------- Model Definition -----------
vocab_size = np.max(X_text_train) + 1
embed_dim = 584
lstm_hidden = 64
num_classes = len(rating_map)
num_numeric_features = X_num_train.shape[1]

# Text input
text_input = Input(shape=(X_text_train.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden, return_sequences=True)(x)
x = AttentionLayer()(x)  # Now supports masking

# Numerical input
num_input = Input(shape=(num_numeric_features,), name='num_input')

# Merge and Dense layers
concat = Concatenate()([x, num_input])
x = Dense(64)(concat)
x = ReLU()(x)
x = Dropout(0.5)(x)
output = Dense(num_classes, activation='softmax')(x)

# Compile model
model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

"""Hyperparameters tuning

"""

#embedding dimensions
embed_dims = [512, 584, 600]
results = []

for dim in embed_dims:
    print(f"\nTesting embed_dim={dim}")
    model = build_model(embed_dim=dim)
    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    history = model.fit(
        [X_text_train, X_num_train], y_train,
        validation_data=([X_text_val, X_num_val], y_val),
        epochs=10,
        batch_size=32,
        callbacks=[early_stop],
        verbose=0
    )
    best_val_acc = max(history.history['val_accuracy'])
    results.append((dim, best_val_acc))

print("Best embedding dim:", max(results, key=lambda x: x[1]))

print(results)

"""## Training"""

#Training

# ----------- EarlyStopping Callback -----------
early_stop = EarlyStopping(
    monitor='val_loss',        # Watch validation loss
    patience=15,                # Wait 3 epochs with no improvement
    restore_best_weights=True,  # Go back to best model weights
    verbose=1
)

# ----------- Train Model (No Class Weights) -----------
history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop]
)

!pip install scikit-learn
from sklearn.metrics import f1_score, classification_report

# Get model predictions on the validation set
y_val_pred_probs = model.predict([X_text_val, X_num_val])
y_val_pred = y_val_pred_probs.argmax(axis=1)

# Compute F1 score (macro = equal weight to each class)
f1 = f1_score(y_val, y_val_pred, average='macro')
print(f"\nüîç Macro F1 Score on Validation Set: {f1:.4f}")

# Optional: full classification report
print("\nüìã Classification Report:")
print(classification_report(y_val, y_val_pred))
# Get model predictions on the validation set
y_val_pred_probs = model.predict([X_text_val, X_num_val])
y_val_pred = y_val_pred_probs.argmax(axis=1)

# Compute F1 score (macro = equal weight to each class)
f1 = f1_score(y_val, y_val_pred, average='macro')
print(f"\nüîç Macro F1 Score on Validation Set: {f1:.4f}")

# Optional: full classification report
print("\nüìã Classification Report:")
print(classification_report(y_val, y_val_pred))

import matplotlib.pyplot as plt

# Plot training & validation loss values
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""Epoch are FAST to run

Training accuracy and validation loss are getting better BUT val acc and val loss are increasing consistently.Reasons -> OVERFITING, learning rate too high, data leakage, label noise or class imbalence, wrong activation or loss


OVERFITTING BADLY

solutions:

1. decrease leanrning rate
2. Increase dropout rate
3. reduce hidden size and embedded dimensions
4. Add regularisation
"""

import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Layer

# ----------- Mask-Supporting Attention Layer -----------
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(shape=(input_shape[-1], 1),
                                 initializer='glorot_uniform',
                                 trainable=True)
        self.b = self.add_weight(shape=(1,),
                                 initializer='zeros',
                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs, mask=None):
        score = K.tanh(K.dot(inputs, self.W) + self.b)  # (batch_size, seq_len, 1)
        score = K.squeeze(score, axis=-1)               # (batch_size, seq_len)

        # Apply the mask: set padded tokens to a large negative value
        if mask is not None:
            score -= 1e9 * (1.0 - K.cast(mask, K.floatx()))

        attention_weights = K.softmax(score, axis=1)              # (batch_size, seq_len)
        attention_weights = K.expand_dims(attention_weights, -1)  # (batch_size, seq_len, 1)

        context_vector = attention_weights * inputs               # (batch_size, seq_len, units)
        context_vector = K.sum(context_vector, axis=1)            # (batch_size, units)

        return context_vector

    def compute_mask(self, inputs, mask=None):
        # Output is a single context vector, so no downstream mask is needed
        return None

# ----------- Model Definition -----------
vocab_size = np.max(X_text_train) + 1
embed_dim = 600
lstm_hidden = 64
num_classes = len(rating_map)
num_numeric_features = X_num_train.shape[1]

# Text input
text_input = Input(shape=(X_text_train.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden, return_sequences=True)(x)
x = AttentionLayer()(x)  # Now supports masking

# Numerical input
num_input = Input(shape=(num_numeric_features,), name='num_input')

# Merge and Dense layers
concat = Concatenate()([x, num_input])
x = Dense(64)(concat)
x = ReLU()(x)
x = Dropout(0.5)(x)
output = Dense(num_classes, activation='softmax')(x)

# Compile model
model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

#Training

# ----------- EarlyStopping Callback -----------
early_stop = EarlyStopping(
    monitor='val_loss',        # Watch validation loss
    patience=15,                # Wait 3 epochs with no improvement
    restore_best_weights=True,  # Go back to best model weights
    verbose=1
)

# ----------- Train Model (No Class Weights) -----------
history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop]
)

"""‚úÖ Positive Observations

The reduced learning rate helped smooth the training curve:

Training accuracy improved steadily (no sharp jumps).

Training loss decreased gradually, showing stable convergence.

‚ùå But Overfitting Still Happens

Validation accuracy stagnates, never exceeding ~0.48.

Validation loss increases significantly after epoch 5, indicating:

The model is overconfident on wrong predictions.

It's memorizing training data rather than generalizing.

# Lower embedded layers
"""

import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Layer

# ----------- Mask-Supporting Attention Layer -----------
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(shape=(input_shape[-1], 1),
                                 initializer='glorot_uniform',
                                 trainable=True)
        self.b = self.add_weight(shape=(1,),
                                 initializer='zeros',
                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs, mask=None):
        score = K.tanh(K.dot(inputs, self.W) + self.b)  # (batch_size, seq_len, 1)
        score = K.squeeze(score, axis=-1)               # (batch_size, seq_len)

        # Apply the mask: set padded tokens to a large negative value
        if mask is not None:
            score -= 1e9 * (1.0 - K.cast(mask, K.floatx()))

        attention_weights = K.softmax(score, axis=1)              # (batch_size, seq_len)
        attention_weights = K.expand_dims(attention_weights, -1)  # (batch_size, seq_len, 1)

        context_vector = attention_weights * inputs               # (batch_size, seq_len, units)
        context_vector = K.sum(context_vector, axis=1)            # (batch_size, units)

        return context_vector

    def compute_mask(self, inputs, mask=None):
        # Output is a single context vector, so no downstream mask is needed
        return None

# ----------- Model Definition -----------
vocab_size = np.max(X_text_train) + 1
embed_dim = 512
lstm_hidden = 64
num_classes = len(rating_map)
num_numeric_features = X_num_train.shape[1]

# Text input
text_input = Input(shape=(X_text_train.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden, return_sequences=True)(x)
x = AttentionLayer()(x)  # Now supports masking

# Numerical input
num_input = Input(shape=(num_numeric_features,), name='num_input')

# Merge and Dense layers
concat = Concatenate()([x, num_input])
x = Dense(64)(concat)
x = ReLU()(x)
x = Dropout(0.6)(x)
output = Dense(num_classes, activation='softmax')(x)

# Compile model
model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

#Training

# ----------- EarlyStopping Callback -----------
early_stop = EarlyStopping(
    monitor='val_loss',        # Watch validation loss
    patience=15,                # Wait 3 epochs with no improvement
    restore_best_weights=True,  # Go back to best model weights
    verbose=1
)

# ----------- Train Model (No Class Weights) -----------
history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop]
)
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True,
    verbose=1
)

history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=25,
    batch_size=16,  # üëà Reduced batch size here
    callbacks=[early_stop],
    verbose=1
)

#Training

# ----------- EarlyStopping Callback -----------
early_stop = EarlyStopping(
    monitor='val_loss',        # Watch validation loss
    patience=15,                # Wait 3 epochs with no improvement
    restore_best_weights=True,  # Go back to best model weights
    verbose=1
)

# ----------- Train Model (No Class Weights) -----------
history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=50,
    batch_size=16,
    callbacks=[early_stop]
)
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True,
    verbose=1
)

"""# Model B: title + abstract + summary + strengths + weaknesses + confidence"""

# Use the scaled dataframes instead of the original ones
X_num_train_df = df_train_scaled[['Confidence']]
X_num_val_df   = df_vali_scaled[['Confidence']]

# Convert to NumPy arrays
X_num_train = X_num_train_df.values.astype(np.float32)
X_num_val   = X_num_val_df.values.astype(np.float32)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam

# ----------- Model with Confidence Input (No Attention) -----------
vocab_size = np.max(X_text_train) + 1
embed_dim = 600
lstm_hidden = 64
num_classes = len(rating_map)

# Text input
text_input = Input(shape=(X_text_train.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden)(x)  # Removed return_sequences and attention

# Single numerical input: Confidence
num_input = Input(shape=(1,), name='num_input')  # just one feature

# Concatenate and build final layers
concat = Concatenate()([x, num_input])
x = Dense(64)(concat)
x = ReLU()(x)
x = Dropout(0.6)(x)
output = Dense(num_classes, activation='softmax')(x)

# Compile
model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import f1_score, classification_report
import matplotlib.pyplot as plt

# --------------------------
# 3. Training
# --------------------------

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

# --------------------------
# 4. Evaluation
# --------------------------

# Predict on validation set
y_val_pred = model.predict([X_text_val, X_num_val], verbose=0).argmax(axis=1)

# Macro F1 score (treats all classes equally)
f1 = f1_score(y_val, y_val_pred, average='macro')
print(f"\nüîç Macro F1 Score: {f1:.4f}")

# Classification report
print("\nüìã Classification Report:")
print(classification_report(y_val, y_val_pred))

# --------------------------
# 5. Visualization
# --------------------------

plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K

# ----------- Attention Layer -----------
class AttentionLayer(Layer):
    def __init__(self, return_attention=False, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
        self.return_attention = return_attention

    def build(self, input_shape):
        self.W = self.add_weight(shape=(input_shape[-1], 1),
                                 initializer='glorot_uniform',
                                 trainable=True)
        self.b = self.add_weight(shape=(1,),
                                 initializer='zeros',
                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs, mask=None):
        score = K.tanh(K.dot(inputs, self.W) + self.b)
        score = K.squeeze(score, axis=-1)
        if mask is not None:
            score -= 1e9 * (1.0 - K.cast(mask, K.floatx()))
        attention_weights = K.softmax(score, axis=1)
        attention_weights = K.expand_dims(attention_weights, -1)
        context_vector = attention_weights * inputs
        context_vector = K.sum(context_vector, axis=1)
        if self.return_attention:
            return [context_vector, attention_weights]
        return context_vector

# ----------- Model with Confidence Input -----------
vocab_size = np.max(X_text_train) + 1
embed_dim = 600
lstm_hidden = 64
num_classes = len(rating_map)

# Text input
text_input = Input(shape=(X_text_train.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden, return_sequences=True)(x)
x = AttentionLayer()(x)

# Single numerical input: Confidence
num_input = Input(shape=(1,), name='num_input')  # just one feature

# Concatenate and build final layers
concat = Concatenate()([x, num_input])
x = Dense(64)(concat)
x = ReLU()(x)
x = Dropout(0.6)(x)
output = Dense(num_classes, activation='softmax')(x)

# Compile
model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

#Training

# ----------- EarlyStopping Callback -----------
early_stop = EarlyStopping(
    monitor='val_loss',        # Watch validation loss
    patience=2,                # Wait 3 epochs with no improvement
    restore_best_weights=True,  # Go back to best model weights
    verbose=1
)

# ----------- Train Model (No Class Weights) -----------
history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=25,
    batch_size=32,
    callbacks=[early_stop]
)

"""1. Performance Decreased Slightly After Dropping Variables

    Training accuracy increased more slowly (now stops at ~57%).

    Validation accuracy peaked at ~41.6% (lower than previous runs with more variables).

    Validation loss initially improved but worsened after epoch 4.

2. EarlyStopping Kicked In Correctly

    It restored the best model from epoch 4, where val accuracy peaked.

3. Removing Variables Reduced Predictive Power

    The dropped variables (soundness, presentation, contribution) were likely informative for classification.

    Keeping only Confidence resulted in slightly less discriminative input.

# MODEL C: summary + soundness + presentation + contribution + strengths + weaknesses + confidence
(without title and abstract)
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# ---------------------------------------
# Step 1: Split the data (Stratified)
# ---------------------------------------

# Stratified split: 70% train, 30% temp
df_train_v2, df_temp_v2 = train_test_split(
    df_train_processed_without,
    test_size=0.30,
    stratify=df_train_processed_without['Rating'],
    random_state=42
)

# Split temp into 20% validation and 10% test
df_vali_v2, df_test_v2 = train_test_split(
    df_temp_v2,
    test_size=0.33,  # ‚âà 10% of original total
    stratify=df_temp_v2['Rating'],
    random_state=42
)

# ---------------------------------------
# Step 2: Feature Scaling (on numeric features)
# ---------------------------------------

# Columns to scale
numcl = ['Soundness', 'Presentation', 'Contribution', 'Confidence']

# Initialize the scaler
scaler_v2 = MinMaxScaler()

# Scale training set
df_train_scaled_v2 = df_train_v2.copy()
df_train_scaled_v2[numcl] = scaler_v2.fit_transform(df_train_v2[numcl])

# Scale validation and test sets using the same scaler
df_vali_scaled_v2 = df_vali_v2.copy()
df_vali_scaled_v2[numcl] = scaler_v2.transform(df_vali_v2[numcl])

df_test_scaled_v2 = df_test_v2.copy()
df_test_scaled_v2[numcl] = scaler_v2.transform(df_test_v2[numcl])

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Initialize and fit tokenizer on the scaled training data (v2)
tokenizer_v2 = Tokenizer()
tokenizer_v2.fit_on_texts(df_train_scaled_v2['combined_tokens_without'])

# Convert text to sequences using the same tokenizer
X_text_train_v2 = tokenizer_v2.texts_to_sequences(df_train_scaled_v2['combined_tokens_without'])
X_text_val_v2   = tokenizer_v2.texts_to_sequences(df_vali_scaled_v2['combined_tokens_without'])

# Pad sequences to the same max length
maxlen_v2 = 512  # adjust as needed
X_text_train_v2 = pad_sequences(X_text_train_v2, padding='post', truncating='post', maxlen=maxlen_v2)
X_text_val_v2   = pad_sequences(X_text_val_v2, padding='post', truncating='post', maxlen=maxlen_v2)

y_train_v2 = df_train_scaled_v2['Rating'].values.astype(np.int32)
y_val_v2   = df_vali_scaled_v2['Rating'].values.astype(np.int32)
y_test_v2  = df_test_scaled_v2['Rating'].values.astype(np.int32)

# (If you haven‚Äôt already, also define your numeric inputs:)
X_num_train_v2 = df_train_scaled_v2[numcl].values.astype(np.float32)
X_num_val_v2   = df_vali_scaled_v2[numcl].values.astype(np.float32)
X_num_test_v2  = df_test_scaled_v2[numcl].values.astype(np.float32)

"""# Scaling error of the Y label coming up
You need to ensure your v2 splits carry zero-based, contiguous class labels before training. Right now df_train_scaled_v2['Rating'] still holds original values like [1,3,5‚Ä¶]. Let‚Äôs fix that by encoding them before the split (or immediately after) and then re-splitting.
"""

from sklearn.preprocessing import LabelEncoder

# 0) On your full ‚Äúwithout title/abstract‚Äù DataFrame:
df_full = df_train_processed_without.copy()

# Encode Rating ‚Üí 0..(num_classes‚àí1)
le = LabelEncoder()
df_full['Rating_enc'] = le.fit_transform(df_full['Rating'])
# Now df_full['Rating_enc'] contains 0,1,2,... with no gaps

# 1) Split df_full (use Rating_enc for stratify)
df_train_v2, df_temp_v2 = train_test_split(
    df_full,
    test_size=0.30,
    stratify=df_full['Rating_enc'],
    random_state=42
)
df_vali_v2, df_test_v2 = train_test_split(
    df_temp_v2,
    test_size=0.33,
    stratify=df_temp_v2['Rating_enc'],
    random_state=42
)

# 2) Scale numeric features as before
scaler_v2 = MinMaxScaler()
df_train_scaled_v2 = df_train_v2.copy()
df_train_scaled_v2[numcl] = scaler_v2.fit_transform(df_train_v2[numcl])
df_vali_scaled_v2 = df_vali_v2.copy()
df_vali_scaled_v2[numcl] = scaler_v2.transform(df_vali_v2[numcl])
df_test_scaled_v2 = df_test_v2.copy()
df_test_scaled_v2[numcl] = scaler_v2.transform(df_test_v2[numcl])

# 3) Pull out y‚Äôs from the encoded column
y_train_v2 = df_train_scaled_v2['Rating_enc'].values.astype(np.int32)
y_val_v2   = df_vali_scaled_v2['Rating_enc'].values.astype(np.int32)
y_test_v2  = df_test_scaled_v2['Rating_enc'].values.astype(np.int32)

# 4) Now re-run the fit check‚Äî
print("y_train_v2 unique:", np.unique(y_train_v2))
# should be [0,1,2,...,num_classes‚àí1]

"""# Model C without Hyperparameter tuning"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import f1_score, classification_report
import numpy as np

# --------------------------
# 1. Model Configuration
# --------------------------

# Dimensions
vocab_size_v2 = np.max(X_text_train_v2) + 1  # based on tokenized sequences
embed_dim = 584
lstm_hidden = 64
num_classes_v2 = len(np.unique(y_train_v2))  # make sure y_train_v2 is defined
num_numeric_features_v2 = X_num_train_v2.shape[1]

# --------------------------
# 2. Model Architecture
# --------------------------

# Text input
text_input_v2 = Input(shape=(X_text_train_v2.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size_v2, output_dim=embed_dim, mask_zero=True)(text_input_v2)
x = LSTM(lstm_hidden)(x)

# Numeric input
num_input_v2 = Input(shape=(num_numeric_features_v2,), name='num_input')

# Concatenate and pass through dense layers
concat = Concatenate()([x, num_input_v2])
x = Dense(64)(concat)
x = ReLU()(x)
x = Dropout(0.5)(x)
output_v2 = Dense(num_classes_v2, activation='softmax')(x)

# Build and compile the model
model_v2 = Model(inputs=[text_input_v2, num_input_v2], outputs=output_v2)
model_v2.compile(optimizer=Adam(learning_rate=0.0001),
                 loss='sparse_categorical_crossentropy',
                 metrics=['accuracy'])

model_v2.summary()

# --------------------------
# 3. Training
# --------------------------

early_stop_v2 = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

history_v2 = model_v2.fit(
    [X_text_train_v2, X_num_train_v2], y_train_v2,
    validation_data=([X_text_val_v2, X_num_val_v2], y_val_v2),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop_v2],
    verbose=1
)

from sklearn.metrics import f1_score
import matplotlib.pyplot as plt

# --------------------------
# 1. F1 Score Calculation
# --------------------------

# Predict class probabilities on the v2 validation set
y_val_pred_v2 = model_v2.predict(
    [X_text_val_v2, X_num_val_v2],
    verbose=0
).argmax(axis=1)

# Compute macro F1 score
f1_v2 = f1_score(y_val_v2, y_val_pred_v2, average='macro')
print(f"\nüîç Macro F1 Score (v2): {f1_v2:.4f}")

# --------------------------
# 2. Loss Curve Plot
# --------------------------

plt.figure(figsize=(10, 5))

# Plot training and validation loss from history_v2
plt.plot(history_v2.history['loss'], label='Training Loss (v2)', linewidth=2)
plt.plot(history_v2.history['val_loss'], label='Validation Loss (v2)', linewidth=2)

# Add titles and labels
plt.title(f'Loss Over Epochs (v2) ‚Äî Macro F1 = {f1_v2:.4f}', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Classification report
print("\nClassification Report:")
print(classification_report(y_val_v2, y_val_pred_v2))

from sklearn.metrics import confusion_matrix

# 1. Predict on the v2 validation set
y_val_pred_v2 = model_v2.predict(
    [X_text_val_v2, X_num_val_v2],
    verbose=0
).argmax(axis=1)

# 2. Compute raw confusion matrix
cm_v2 = confusion_matrix(y_val_v2, y_val_pred_v2)
print("Confusion Matrix (counts):\n", cm_v2)

# 3. (Optional) Normalize it row-wise so each true class sums to 1
cm_v2_norm = cm_v2.astype(float) / cm_v2.sum(axis=1)[:, np.newaxis]

# 4. Plot the normalized confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(
    cm_v2_norm,
    annot=True, fmt=".2f",
    cmap="Blues",
    xticklabels=le.classes_,  # or range(num_classes_v2) if you don't have le
    yticklabels=le.classes_
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Normalized Confusion Matrix (Validation)")
plt.show()

"""# MODEL C with Hyper Parameter Tuning"""

import shutil; shutil.rmtree('rnn_tuning_v2', ignore_errors=True)

from keras_tuner import HyperModel, RandomSearch
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import tensorflow as tf # Import TensorFlow to potentially check GPU/CuDNN later if needed

# 1) Define your hyperparameter ranges
EMBED_DIM_RANGE     = (448, 640, 64)
LSTM_HIDDEN_RANGE   = (32, 128, 32)
DENSE_UNITS_RANGE   = (32, 128, 32)
DROPOUT_RANGE       = (0.3, 0.7, 0.1)
LEARNING_RATE_OPTIONS = [1e-3, 1e-4]

# 2) Create a HyperModel subclass that uses Model C dimensions
class TextNumericHyperModel(HyperModel):
    def __init__(self, vocab_size, text_len, num_num_feats, num_classes):
        self.vocab_size = vocab_size
        self.text_len = text_len
        self.num_num_feats = num_num_feats
        self.num_classes = num_classes

    def build(self, hp):
        embed_dim   = hp.Int('embed_dim',   *EMBED_DIM_RANGE)
        lstm_units  = hp.Int('lstm_units',  *LSTM_HIDDEN_RANGE)
        dense_units = hp.Int('dense_units', *DENSE_UNITS_RANGE)
        drop_rate   = hp.Float('dropout',   *DROPOUT_RANGE)
        lr          = hp.Choice('lr',       LEARNING_RATE_OPTIONS)

        # text branch
        txt_in = Input((self.text_len,), name='text_input')
        x = Embedding(self.vocab_size, embed_dim, mask_zero=True)(txt_in)
        # Add use_cudnn=False to the LSTM layer
        x = LSTM(lstm_units, use_cudnn=False)(x)

        # numeric branch
        num_in = Input((self.num_num_feats,), name='num_input')
        x = Concatenate()([x, num_in])
        x = Dense(dense_units)(x)
        x = ReLU()(x)
        x = Dropout(drop_rate)(x)
        out = Dense(self.num_classes, activation='softmax')(x)

        model = Model([txt_in, num_in], out)
        model.compile(
            optimizer=Adam(lr),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

# 3) Instantiate it with your v2 shapes
# Assuming X_text_train_v2, X_num_train_v2, y_train_v2, X_text_val_v2, X_num_val_v2, y_val_v2 are defined earlier
# Ensure these variables are correctly defined based on your Model C data splits
# Based on global variables, these seem to be defined.
hypermodel_v2 = TextNumericHyperModel(
    vocab_size       = np.max(X_text_train_v2) + 1,
    text_len         = X_text_train_v2.shape[1],
    num_num_feats    = X_num_train_v2.shape[1],
    num_classes      = len(np.unique(y_train_v2))
)

# 4) Configure RandomSearch
# Consider clearing the previous tuner directory if you had failed runs
# import shutil
# shutil.rmtree('rnn_tuning_v2', ignore_errors=True)
tuner_v2 = RandomSearch(
    hypermodel_v2,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='rnn_tuning_v2',
    project_name='text_num_lstm'
)

# 5) Run the search on your Model C splits
tuner_v2.search(
    [X_text_train_v2, X_num_train_v2], y_train_v2,
    validation_data=([X_text_val_v2, X_num_val_v2], y_val_v2),
    epochs=3, # Keep a low number of epochs for tuning trials
    batch_size=32,
    callbacks=[EarlyStopping(monitor='val_loss', patience=2)]
)

# 6) Retrieve the best model & hyperparameters
best_model_v2 = tuner_v2.get_best_models(num_models=1)[0]
best_hps_v2   = tuner_v2.get_best_hyperparameters(num_trials=1)[0]

print("Best Model C hyperparameters:")
print(f" ‚Ä¢ embed_dim:  {best_hps_v2.get('embed_dim')}")
print(f" ‚Ä¢ lstm_units: {best_hps_v2.get('lstm_units')}")
print(f" ‚Ä¢ dense_units: {best_hps_v2.get('dense_units')}")
print(f" ‚Ä¢ dropout:     {best_hps_v2.get('dropout')}")
print(f" ‚Ä¢ learning rate: {best_hps_v2.get('lr')}")

"""# Define Accuracy Function"""

# -*- coding: utf-8 -*-
"""Group10QBUS6850_pred_2025S1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f2FUqLeZott4QdeAusEm5Nik9lcIx5tX
"""

from google.colab import drive

drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
import re

pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118

pip install torchtext

import torch
import torch.nn as nn
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torchtext.vocab import vocab
from collections import Counter
torch.manual_seed(0)

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.util import ngrams
from sklearn.feature_extraction.text import TfidfVectorizer

# Setup
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')
lemmatizer = WordNetLemmatizer()

# Initialize tools
tokenizer = get_tokenizer("basic_english")
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#print(f'Using device: {device}')

"""# Read Files + Impute Missing Values"""

df_sub = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Submissions.csv')
df_sub.head(5)

df_test_original = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Rating_Test.csv')
df_test_original.head(5)

df_train_original = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Rating_Train.csv')
df_train_original .head(5)

# Check for missing values in the training, validation, and test data
print("Missing values in submissions data:")
print(df_sub.isnull().sum())

# Check basic information about the datasets (e.g., data types, number of non-null entries)
print("\nSubmissions data info:")
print(df_sub.info())

# Check for duplicate rows (excluding ID columns)
print("\nDuplicate rows in training data (excluding review_ID_train):",
      df_sub.duplicated(subset=[col for col in df_sub.columns if col != 'reviewID_train']).sum())

# Summary statistics to understand the distribution of numerical columns
print("\nSubmissions data summary statistics:")
print(df_sub.describe())

# Drop TLDR column
df_sub.drop(columns=['tldr'], inplace=True)

# Check for missing values in the training, validation, and test data
print("Missing values in training data:")
print(df_train_original.isnull().sum())

# Check basic information about the datasets (e.g., data types, number of non-null entries)
print("\nTraining data info:")
print(df_train_original.info())

# Check for duplicate rows (excluding ID columns)
print("\nDuplicate rows in training data (excluding review_ID_train):",
      df_train_original.duplicated(subset=[col for col in df_train_original.columns if col != 'reviewID_train']).sum())

# Summary statistics to understand the distribution of numerical columns
print("\nTraining data summary statistics:")
print(df_train_original.describe())

# Impute missing values with 'Not provided' for Strengths and Weaknesses and Summary in ICLR_train, ICLR_vali, and ICLR_test
df_train_original['Strengths'] = df_train_original['Strengths'].fillna('Not provided')
df_train_original['Weaknesses'] = df_train_original['Weaknesses'].fillna('Not provided')
df_train_original['Summary'] = df_train_original['Summary'].fillna('Not provided')

# Check whether missing values are gone
print("Missing values in training data:")
print(df_train_original[['Strengths', 'Weaknesses','Summary']].isnull().sum())

"""# Text Pre-Processing

1. Text Cleaning

* convert to lowercase
* remove URLs
* remove punctuation, other characters
* remove digits/ numbers
"""

# Define your preprocessing function

# Precompile URL regex
url_pattern = re.compile(r'https?://\S+|www\.\S+')

def clean_text(text):
    text = text.lower() # Convert to lowercase
    text = re.sub(r'[^a-z\s]', ' ', text)  # Remove numbers, punctuation, special chars
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    return text

# Columns to process
text_columns = ['title', 'abstract', 'Summary', 'Strengths', 'Weaknesses']

# Apply preprocessing directly (since no NaNs exist)
for col in ['title', 'abstract', 'Summary', 'Strengths', 'Weaknesses']:
    df_train_original[col] = df_train_original[col].apply(clean_text)

"""2. Tokenization"""

# Initialize basic English tokenizer
tokenizer = get_tokenizer("basic_english")

# Tokenize each text column
for col in ['title', 'abstract', 'Summary', 'Strengths', 'Weaknesses']:
    token_col = col + '_tokens'
    df_train_original[token_col] = df_train_original[col].apply(tokenizer)

"""3. Stop-Word Removal --> NLTK"""

# Load English stopwords
stop_words = set(stopwords.words('english'))

# Remove stopwords from tokenized columns
for col in ['title_tokens', 'abstract_tokens', 'Summary_tokens', 'Strengths_tokens', 'Weaknesses_tokens']:
    df_train_original[col] = df_train_original[col].apply(lambda tokens: [token for token in tokens if token not in stop_words])

"""4. Lemmatization --> WordNetLemmatizer & POS_tag"""

#import nltk
from nltk.corpus import wordnet
from nltk import pos_tag

# Download required NLTK resources
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to convert POS tag to WordNet format
def get_wordnet_pos(word):
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {
        'J': wordnet.ADJ,
        'N': wordnet.NOUN,
        'V': wordnet.VERB,
        'R': wordnet.ADV
    }
    return tag_dict.get(tag, wordnet.NOUN)

# Function to lemmatize a list of tokens using POS
def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]

for col in ['title_tokens', 'abstract_tokens', 'Summary_tokens', 'Strengths_tokens', 'Weaknesses_tokens']:
    df_train_original[col] = df_train_original[col].apply(lemmatize_tokens)

df_train_original.head(2)

"""saving pre-processed df_train_original to a csv"""

df_train_original.to_csv("df_train_original_processed.csv", index=False)

"""# Training data Pre-processed CSV file"""

df_train_processed= pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/df_train_original_processed.csv')

df_train_processed_without = df_train_processed.drop(columns=['title', 'abstract','title_tokens','abstract_tokens'])
df_train_processed_without.head(2)

"""### combining tokens"""

# Combine tokens into one list per row
df_train_processed['combined_tokens'] = df_train_processed['title_tokens'] + df_train_processed['abstract_tokens'] + df_train_processed['Summary_tokens'] + df_train_processed['Strengths_tokens'] + df_train_processed['Weaknesses_tokens']

df_train_processed.head(5)

df_train_processed_without['combined_tokens_without'] = df_train_processed['Summary_tokens'] + df_train_processed['Strengths_tokens'] + df_train_processed['Weaknesses_tokens']

df_train_processed_without.head(2)

"""# Train-Vali-Split"""

# Step 1: 70% train, 30% temp (stratified)
df_train, df_temp = train_test_split(
    df_train_processed,
    test_size=0.30,
    stratify=df_train_processed['Rating'],
    random_state=42
)

# Step 2: Split temp into 20% validation and 10% test
# Use stratify again to preserve distribution
df_vali, df_test = train_test_split(
    df_temp,
    test_size=0.33,  # 33% of 30% ‚âà 10% of total
    stratify=df_temp['Rating'],
    random_state=42
)

print("Train shape:", df_train.shape)
print("Validation shape:", df_vali.shape)
print("Test shape:", df_test.shape)

"""# Feature Engineering"""

# Step 3: Feature Scaling (X)
# ----------------------------
from sklearn.preprocessing import MinMaxScaler

# Columns to scale
numcl = ['Soundness', 'Presentation', 'Contribution', 'Confidence']

# Initialize MinMaxScaler (default scales to [0, 1])
scaler = MinMaxScaler()

# Fit scaler on training data only
df_train_scaled = df_train.copy()
df_train_scaled[numcl] = scaler.fit_transform(df_train[numcl])

# Transform validation and test sets with the same scaler
df_vali_scaled = df_vali.copy()
df_vali_scaled[numcl] = scaler.transform(df_vali[numcl])

df_test_scaled = df_test.copy()
df_test_scaled[numcl] = scaler.transform(df_test[numcl])

# Rating  0 ~ num_classes-1
all_labels = sorted(df_train_processed['Rating'].unique())
rating_map = {v: i for i, v in enumerate(all_labels)}

for df in [df_train, df_vali, df_test]:
    df['Rating'] = df['Rating'].map(rating_map)

df_train.head(2)

df_train_scaled.head(2)

"""# Modelling

## RNN and LSTM Models Comparision
"""

textcol = ['combined_tokens']
numcl = ['Soundness','Presentation','Contribution','Confidence']
y = ['Rating']



from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Fit tokenizer on the full text corpus
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df_train['combined_tokens'])

# Convert texts to sequences
train_seq = tokenizer.texts_to_sequences(df_train['combined_tokens'])
val_seq = tokenizer.texts_to_sequences(df_vali['combined_tokens'])
test_seq = tokenizer.texts_to_sequences(df_test['combined_tokens'])

# Pad sequences
X_text_train = pad_sequences(train_seq, padding='post', maxlen=300)
X_text_val   = pad_sequences(val_seq, padding='post', maxlen=300)
X_text_test  = pad_sequences(test_seq, padding='post', maxlen=300)

# Use scaled versions for numeric input features
X_num_train = df_train_scaled[numcl].values.astype(np.float32)
X_num_val   = df_vali_scaled[numcl].values.astype(np.float32)
X_num_test  = df_test_scaled[numcl].values.astype(np.float32)

# Use the same (scaled) DataFrames for labels if Rating mapping was applied to them
y_train = df_train_scaled['Rating'].values.astype(np.int32)
y_val   = df_vali_scaled['Rating'].values.astype(np.int32)
y_test  = df_test_scaled['Rating'].values.astype(np.int32)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_val = le.transform(y_val)
y_test = le.transform(y_test)

# Optional: check new unique values
print("y_train after encoding:", np.unique(y_train))

print("Max token index in X_text_train:", np.max(X_text_train))



"""### RNN"""

import shutil
shutil.rmtree('rnn_tuning', ignore_errors=True)

!pip install keras-tuner -q

# import shutil
import random
import tensorflow as tf
from keras_tuner import HyperModel, RandomSearch
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping


# shutil.rmtree('rnn_tuning', ignore_errors=True)

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

EMBED_DIM_RANGE = (448, 640, 64)
RNN_HIDDEN_RANGE = (32, 128, 32)
DENSE_UNITS_RANGE = (32, 128, 32)
DROPOUT_RANGE = (0.3, 0.7, 0.1)
LEARNING_RATE_OPTIONS = [1e-3, 1e-4]


class TextNumericHyperModel(HyperModel):
    def __init__(self, vocab_size, input_text_len, num_numeric_features, num_classes):
        self.vocab_size = vocab_size
        self.input_text_len = input_text_len
        self.num_numeric_features = num_numeric_features
        self.num_classes = num_classes

    def build(self, hp):
        embed_dim = hp.Int('embed_dim', *EMBED_DIM_RANGE)
        rnn_hidden = hp.Int('rnn_hidden', *RNN_HIDDEN_RANGE)
        dense_units = hp.Int('dense_units', *DENSE_UNITS_RANGE)
        dropout_rate = hp.Float('dropout', *DROPOUT_RANGE)
        learning_rate = hp.Choice('lr', LEARNING_RATE_OPTIONS)

        text_input = Input(shape=(self.input_text_len,), name='text_input')
        x = Embedding(input_dim=self.vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
        x = SimpleRNN(rnn_hidden)(x)

        num_input = Input(shape=(self.num_numeric_features,), name='num_input')
        concat = Concatenate()([x, num_input])
        x = Dense(dense_units)(concat)
        x = ReLU()(x)
        x = Dropout(dropout_rate)(x)
        output = Dense(self.num_classes, activation='softmax')(x)

        model = Model(inputs=[text_input, num_input], outputs=output)
        model.compile(
            optimizer=Adam(learning_rate),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model


hypermodel = TextNumericHyperModel(
    vocab_size=np.max(X_text_train) + 1,
    input_text_len=X_text_train.shape[1],
    num_numeric_features=X_num_train.shape[1],
    num_classes=len(rating_map)
)


tuner = RandomSearch(
    hypermodel,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    seed=SEED,
    directory='rnn_tuning',
    project_name='text_num_rnn'
)


tuner.search(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=3,
    batch_size=32,
    callbacks=[EarlyStopping(monitor='val_loss', patience=2)]
)


best_model = tuner.get_best_models(num_models=1)[0]


best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters:")
print(f"Embedding dimension: {best_hps.get('embed_dim')}")
print(f"RNN hidden units: {best_hps.get('rnn_hidden')}")
print(f"Dense units: {best_hps.get('dense_units')}")
print(f"Dropout rate: {best_hps.get('dropout')}")
print(f"Learning rate: {best_hps.get('lr')}")

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import classification_report, f1_score


checkpoint_path = "best_model.weights.h5"

callbacks = [
    EarlyStopping(monitor='val_loss', patience=12, verbose=1),
    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)
]

history = best_model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=30,
    batch_size=32,
    callbacks=callbacks
)


best_model.load_weights(checkpoint_path)


y_pred_probs = best_model.predict([X_text_val, X_num_val])
y_pred = np.argmax(y_pred_probs, axis=1)

print(classification_report(y_val, y_pred))
weighted_f1 = f1_score(y_val, y_pred, average='weighted')
print(f"Weighted F1-score (best epoch): {weighted_f1:.4f}")



"""### LSTM"""

import shutil
import random
import tensorflow as tf
from keras_tuner import HyperModel, RandomSearch
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping


shutil.rmtree('rnn_tuning', ignore_errors=True)

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

EMBED_DIM_RANGE = (448, 640, 64)
LSTM_HIDDEN_RANGE = (32, 128, 32)
DENSE_UNITS_RANGE = (32, 128, 32)
DROPOUT_RANGE = (0.3, 0.7, 0.1)
LEARNING_RATE_OPTIONS = [1e-3, 1e-4]


class TextNumericHyperModel(HyperModel):
    def __init__(self, vocab_size, input_text_len, num_numeric_features, num_classes):
        self.vocab_size = vocab_size
        self.input_text_len = input_text_len
        self.num_numeric_features = num_numeric_features
        self.num_classes = num_classes

    def build(self, hp):
        embed_dim = hp.Int('embed_dim', *EMBED_DIM_RANGE)
        lstm_hidden = hp.Int('lstm_hidden', *LSTM_HIDDEN_RANGE)
        dense_units = hp.Int('dense_units', *DENSE_UNITS_RANGE)
        dropout_rate = hp.Float('dropout', *DROPOUT_RANGE)
        learning_rate = hp.Choice('lr', LEARNING_RATE_OPTIONS)

        text_input = Input(shape=(self.input_text_len,), name='text_input')
        x = Embedding(input_dim=self.vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
        x = LSTM(lstm_hidden)(x)

        num_input = Input(shape=(self.num_numeric_features,), name='num_input')
        concat = Concatenate()([x, num_input])
        x = Dense(dense_units)(concat)
        x = ReLU()(x)
        x = Dropout(dropout_rate)(x)
        output = Dense(self.num_classes, activation='softmax')(x)

        model = Model(inputs=[text_input, num_input], outputs=output)
        model.compile(
            optimizer=Adam(learning_rate),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model


hypermodel = TextNumericHyperModel(
    vocab_size=np.max(X_text_train) + 1,
    input_text_len=X_text_train.shape[1],
    num_numeric_features=X_num_train.shape[1],
    num_classes=len(rating_map)
)

tuner = RandomSearch(
    hypermodel,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=1,
    directory='rnn_tuning',
    project_name='text_num_model'
)

tuner.search(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=3,
    batch_size=32,
    callbacks=[EarlyStopping(monitor='val_loss', patience=2)]
)
best_model = tuner.get_best_models(num_models=1)[0]

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

print("Best Hyperparameters Found:")
print(f"Embedding dimension: {best_hps.get('embed_dim')}")
print(f"LSTM hidden units: {best_hps.get('lstm_hidden')}")
print(f"Dense units: {best_hps.get('dense_units')}")
print(f"Dropout rate: {best_hps.get('dropout')}")
print(f"Learning rate: {best_hps.get('lr')}")

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import classification_report, f1_score
import os


checkpoint_path = "best_model.weights.h5"


callbacks = [
    EarlyStopping(monitor='val_loss', patience=12, verbose=1),
    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)
]


history = best_model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=30,
    batch_size=32,
    callbacks=callbacks
)


best_model.load_weights(checkpoint_path)


y_pred_probs = best_model.predict([X_text_val, X_num_val])
y_pred = np.argmax(y_pred_probs, axis=1)

print(classification_report(y_val, y_pred))
weighted_f1 = f1_score(y_val, y_pred, average='weighted')
print(f"Weighted F1-score (best epoch): {weighted_f1:.4f}")





from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam


vocab_size = np.max(X_text_train) + 1
embed_dim = 584
lstm_hidden = 64
num_classes = len(rating_map)
num_numeric_features = X_num_train.shape[1]


text_input = Input(shape=(X_text_train.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden)(x)


num_input = Input(shape=(num_numeric_features,), name='num_input')


concat = Concatenate()([x, num_input])
x = Dense(64)(concat)
x = ReLU()(x)
x = Dropout(0.5)(x)
output = Dense(num_classes, activation='softmax')(x)


model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

"""## Variable Set Comparision

### MODEL A (all the variables)
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import f1_score, classification_report

vocab_size = np.max(X_text_train) + 1  # assuming X_text_train contains integer-encoded tokens
embed_dim = 640
lstm_hidden = 32
num_classes = len(np.unique(y_train))  # or len(rating_map)
num_numeric_features = X_num_train.shape[1]


# Text input
text_input = Input(shape=(X_text_train.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden)(x)

# Numeric input
num_input = Input(shape=(num_numeric_features,), name='num_input')

# Concatenate text + numeric features
concat = Concatenate()([x, num_input])
x = Dense(96)(concat)
x = ReLU()(x)
x = Dropout(0.3)(x)
output = Dense(num_classes, activation='softmax')(x)

# Build and compile model
model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()


early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)

history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

# Predict on validation set
y_val_pred = model.predict([X_text_val, X_num_val]).argmax(axis=1)

# Macro F1 score (treats all classes equally)
f1 = f1_score(y_val, y_val_pred, average='macro')
print(f"\n Macro F1 Score: {f1:.4f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_val, y_val_pred))

plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""### Model B(Without Soundness, presentation and Contribution)"""

# Use the scaled dataframes instead of the original ones
X_num_train_df = df_train_scaled[['Confidence']]
X_num_val_df   = df_vali_scaled[['Confidence']]

# Convert to NumPy arrays
X_num_train = X_num_train_df.values.astype(np.float32)
X_num_val   = X_num_val_df.values.astype(np.float32)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam

vocab_size = np.max(X_text_train) + 1
embed_dim = 640
lstm_hidden = 32
num_classes = len(rating_map)

# Text input
text_input = Input(shape=(X_text_train.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden)(x)  # Removed return_sequences and attention

# Single numerical input: Confidence
num_input = Input(shape=(1,), name='num_input')  # just one feature

# Concatenate and build final layers
concat = Concatenate()([x, num_input])
x = Dense(96)(concat)
x = ReLU()(x)
x = Dropout(0.3)(x)
output = Dense(num_classes, activation='softmax')(x)

# Compile
model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import f1_score, classification_report
import matplotlib.pyplot as plt


early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

history = model.fit(
    [X_text_train, X_num_train], y_train,
    validation_data=([X_text_val, X_num_val], y_val),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)


# Predict on validation set
y_val_pred = model.predict([X_text_val, X_num_val], verbose=0).argmax(axis=1)

# Macro F1 score (treats all classes equally)
f1 = f1_score(y_val, y_val_pred, average='macro')
print(f"\nüîç Macro F1 Score: {f1:.4f}")

# Classification report
print("\nüìã Classification Report:")
print(classification_report(y_val, y_val_pred))


plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""### Model C(Without abstract and title)"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler


# Stratified split: 70% train, 30% temp
df_train_v2, df_temp_v2 = train_test_split(
    df_train_processed_without,
    test_size=0.30,
    stratify=df_train_processed_without['Rating'],
    random_state=42
)

# Split temp into 20% validation and 10% test
df_vali_v2, df_test_v2 = train_test_split(
    df_temp_v2,
    test_size=0.33,  # ‚âà 10% of original total
    stratify=df_temp_v2['Rating'],
    random_state=42
)


# Columns to scale
numcl = ['Soundness', 'Presentation', 'Contribution', 'Confidence']

# Initialize the scaler
scaler_v2 = MinMaxScaler()

# Scale training set
df_train_scaled_v2 = df_train_v2.copy()
df_train_scaled_v2[numcl] = scaler_v2.fit_transform(df_train_v2[numcl])

# Scale validation and test sets using the same scaler
df_vali_scaled_v2 = df_vali_v2.copy()
df_vali_scaled_v2[numcl] = scaler_v2.transform(df_vali_v2[numcl])

df_test_scaled_v2 = df_test_v2.copy()
df_test_scaled_v2[numcl] = scaler_v2.transform(df_test_v2[numcl])

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Initialize and fit tokenizer on the scaled training data (v2)
tokenizer_v2 = Tokenizer()
tokenizer_v2.fit_on_texts(df_train_scaled_v2['combined_tokens_without'])


# Convert text to sequences using the same tokenizer
X_text_train_v2 = tokenizer_v2.texts_to_sequences(df_train_scaled_v2['combined_tokens_without'])
X_text_val_v2   = tokenizer_v2.texts_to_sequences(df_vali_scaled_v2['combined_tokens_without'])
X_text_test_v2   = tokenizer_v2.texts_to_sequences(df_test_scaled_v2['combined_tokens_without'])

# Pad sequences to the same max length
maxlen_v2 = 512  # adjust as needed
X_text_train_v2 = pad_sequences(X_text_train_v2, padding='post', truncating='post', maxlen=maxlen_v2)
X_text_val_v2   = pad_sequences(X_text_val_v2, padding='post', truncating='post', maxlen=maxlen_v2)
X_text_test_v2   = pad_sequences(X_text_test_v2, padding='post', truncating='post', maxlen=maxlen_v2)

y_train_v2 = df_train_scaled_v2['Rating'].values.astype(np.int32)
y_val_v2   = df_vali_scaled_v2['Rating'].values.astype(np.int32)
y_test_v2  = df_test_scaled_v2['Rating'].values.astype(np.int32)

# (If you haven‚Äôt already, also define your numeric inputs:)
X_num_train_v2 = df_train_scaled_v2[numcl].values.astype(np.float32)
X_num_val_v2   = df_vali_scaled_v2[numcl].values.astype(np.float32)
X_num_test_v2  = df_test_scaled_v2[numcl].values.astype(np.float32)

from sklearn.preprocessing import LabelEncoder

# 0) On your full ‚Äúwithout title/abstract‚Äù DataFrame:
df_full = df_train_processed_without.copy()

# Encode Rating ‚Üí 0..(num_classes‚àí1)
le = LabelEncoder()
df_full['Rating_enc'] = le.fit_transform(df_full['Rating'])
# Now df_full['Rating_enc'] contains 0,1,2,... with no gaps

# 1) Split df_full (use Rating_enc for stratify)
df_train_v2, df_temp_v2 = train_test_split(
    df_full,
    test_size=0.30,
    stratify=df_full['Rating_enc'],
    random_state=42
)
df_vali_v2, df_test_v2 = train_test_split(
    df_temp_v2,
    test_size=0.33,
    stratify=df_temp_v2['Rating_enc'],
    random_state=42
)

# 2) Scale numeric features as before
scaler_v2 = MinMaxScaler()
df_train_scaled_v2 = df_train_v2.copy()
df_train_scaled_v2[numcl] = scaler_v2.fit_transform(df_train_v2[numcl])
df_vali_scaled_v2 = df_vali_v2.copy()
df_vali_scaled_v2[numcl] = scaler_v2.transform(df_vali_v2[numcl])
df_test_scaled_v2 = df_test_v2.copy()
df_test_scaled_v2[numcl] = scaler_v2.transform(df_test_v2[numcl])

# 3) Pull out y‚Äôs from the encoded column
y_train_v2 = df_train_scaled_v2['Rating_enc'].values.astype(np.int32)
y_val_v2   = df_vali_scaled_v2['Rating_enc'].values.astype(np.int32)
y_test_v2  = df_test_scaled_v2['Rating_enc'].values.astype(np.int32)

# 4) Now re-run the fit check‚Äî
print("y_train_v2 unique:", np.unique(y_train_v2))
# should be [0,1,2,...,num_classes‚àí1]

X_text_train_v2 = np.array(X_text_train_v2, dtype=np.int32)  # tokenized sequences, int32 is OK for embedding
X_num_train_v2 = np.array(X_num_train_v2, dtype=np.float32)
y_train_v2 = np.array(y_train_v2).ravel().astype(int)

X_text_val_v2 = np.array(X_text_val_v2, dtype=np.int32)
X_num_val_v2 = np.array(X_num_val_v2, dtype=np.float32)
y_val_v2 = np.array(y_val_v2).ravel().astype(int)

"""#### Model Building"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import f1_score, classification_report
import numpy as np


# Ensure numpy arrays & correct shape
X_text_train_v2 = np.array(X_text_train_v2)
X_num_train_v2 = np.array(X_num_train_v2)
y_train_v2 = np.array(y_train_v2).ravel().astype(int)

X_text_val_v2 = np.array(X_text_val_v2)
X_num_val_v2 = np.array(X_num_val_v2)
y_val_v2 = np.array(y_val_v2).ravel().astype(int)


vocab_size_v2 = np.max(X_text_train_v2) + 1
embed_dim = 640
lstm_hidden = 32
num_classes_v2 = len(np.unique(y_train_v2))
num_numeric_features_v2 = X_num_train_v2.shape[1]


text_input_v2 = Input(shape=(X_text_train_v2.shape[1],), name='text_input')
x = Embedding(input_dim=vocab_size_v2, output_dim=embed_dim, mask_zero=True)(text_input_v2)
x = LSTM(lstm_hidden)(x)

num_input_v2 = Input(shape=(num_numeric_features_v2,), name='num_input')
concat = Concatenate()([x, num_input_v2])
x = Dense(96)(concat)
x = ReLU()(x)
x = Dropout(0.3)(x)
output_v2 = Dense(num_classes_v2, activation='softmax')(x)

model_v2 = Model(inputs=[text_input_v2, num_input_v2], outputs=output_v2)
model_v2.compile(optimizer=Adam(learning_rate=0.001),
                 loss='sparse_categorical_crossentropy',
                 metrics=['accuracy'])

model_v2.summary()


early_stop_v2 = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

history_v2 = model_v2.fit(
    {'text_input': X_text_train_v2, 'num_input': X_num_train_v2}, y_train_v2,
    validation_data=({'text_input': X_text_val_v2, 'num_input': X_num_val_v2}, y_val_v2),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop_v2],
    verbose=1
)



y_val_pred_prob = model_v2.predict([X_text_val_v2, X_num_val_v2])
y_val_pred = np.argmax(y_val_pred_prob, axis=1)

f1 = f1_score(y_val_v2, y_val_pred, average='weighted')
print("Weighted F1 Score:", f1)

print("\nClassification Report:\n")
print(classification_report(y_val_v2, y_val_pred))

from sklearn.metrics import f1_score
import matplotlib.pyplot as plt


# Predict class probabilities on the v2 validation set
y_val_pred_v2 = model_v2.predict(
    [X_text_val_v2, X_num_val_v2],
    verbose=0
).argmax(axis=1)

# Compute macro F1 score
f1_v2 = f1_score(y_val_v2, y_val_pred_v2, average='macro')
print(f"\nüîç Macro F1 Score (v2): {f1_v2:.4f}")


plt.figure(figsize=(10, 5))

# Plot training and validation loss from history_v2
plt.plot(history_v2.history['loss'], label='Training Loss (v2)', linewidth=2)
plt.plot(history_v2.history['val_loss'], label='Validation Loss (v2)', linewidth=2)

# Add titles and labels
plt.title(f'Loss Over Epochs (v2) ‚Äî Macro F1 = {f1_v2:.4f}', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Classification report
print("\nClassification Report:")
print(classification_report(y_val_v2, y_val_pred_v2))

from sklearn.metrics import confusion_matrix

# 1. Predict on the v2 validation set
y_val_pred_v2 = model_v2.predict(
    [X_text_val_v2, X_num_val_v2],
    verbose=0
).argmax(axis=1)

# 2. Compute raw confusion matrix
cm_v2 = confusion_matrix(y_val_v2, y_val_pred_v2)
print("Confusion Matrix (counts):\n", cm_v2)

# 3. (Optional) Normalize it row-wise so each true class sums to 1
cm_v2_norm = cm_v2.astype(float) / cm_v2.sum(axis=1)[:, np.newaxis]

# 4. Plot the normalized confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(
    cm_v2_norm,
    annot=True, fmt=".2f",
    cmap="Blues",
    xticklabels=le.classes_,  # or range(num_classes_v2) if you don't have le
    yticklabels=le.classes_
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Normalized Confusion Matrix (Validation)")
plt.show()

"""## Hyperparameter Tuning for LSTM on Model C(without title and abstract)"""

!pip install keras-tuner



import numpy as np
import os, shutil, random
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import keras_tuner as kt

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

X_text_train_v2 = np.array(X_text_train_v2)
X_text_val_v2 = np.array(X_text_val_v2)
X_num_train_v2 = np.array(X_num_train_v2)
X_num_val_v2 = np.array(X_num_val_v2)
y_train_v2 = np.array(y_train_v2).ravel().astype(int)
y_val_v2 = np.array(y_val_v2).ravel().astype(int)

class FineTunedLSTMHyperModel(kt.HyperModel):
    def build(self, hp):
        embed_dim = hp.Choice('embedding_dim', [576, 640])
        lstm_hidden = hp.Choice('lstm_hidden_units', [32, 48, 64])
        dense_units = hp.Choice('dense_units', [64, 96, 112])
        dropout_rate = hp.Choice('dropout_rate', [0.2, 0.3, 0.4])
        learning_rate = hp.Choice('learning_rate', [1e-3, 5e-4, 1e-4])

        text_input = Input(shape=(X_text_train_v2.shape[1],), name='text_input')
        x = Embedding(input_dim=np.max(X_text_train_v2) + 1, output_dim=embed_dim, mask_zero=True)(text_input)
        x = LSTM(lstm_hidden)(x)

        num_input = Input(shape=(X_num_train_v2.shape[1],), name='num_input')
        concat = Concatenate()([x, num_input])
        x = Dense(dense_units)(concat)
        x = ReLU()(x)
        x = Dropout(dropout_rate)(x)
        output = Dense(len(np.unique(y_train_v2)), activation='softmax')(x)

        model = Model(inputs=[text_input, num_input], outputs=output)
        model.compile(optimizer=Adam(learning_rate),
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])
        return model

    def fit(self, hp, model, *args, **kwargs):
        batch_size = hp.Choice('batch_size', [16, 32, 64])
        return model.fit(*args,
                         batch_size=batch_size,
                         **kwargs)

project_name = "lstm_batch_tune"
shutil.rmtree(os.path.join("lstm_tuning", project_name), ignore_errors=True)

tuner = kt.RandomSearch(
    hypermodel=FineTunedLSTMHyperModel(),
    objective='val_accuracy',
    max_trials=20,
    executions_per_trial=1,
    directory='lstm_tuning',
    project_name=project_name,
    seed=SEED,
    overwrite=True
)

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

tuner.search(
    {'text_input': X_text_train_v2, 'num_input': X_num_train_v2},
    y_train_v2,
    validation_data=({'text_input': X_text_val_v2, 'num_input': X_num_val_v2}, y_val_v2),
    epochs=3,
    callbacks=[early_stop],
    verbose=2
)

best_model = tuner.get_best_models(1)[0]
best_hps = tuner.get_best_hyperparameters(1)[0]

print("Best Hyperparameters:")
for p in ['embedding_dim', 'lstm_hidden_units', 'dense_units', 'dropout_rate', 'learning_rate', 'batch_size']:
    print(f"{p}: {best_hps.get(p)}")



from sklearn.metrics import classification_report
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import numpy as np


embed_dim     = best_hps.get('embedding_dim')
lstm_hidden   = best_hps.get('lstm_hidden_units')
dense_units   = best_hps.get('dense_units')
dropout_rate  = best_hps.get('dropout_rate')
learning_rate = best_hps.get('learning_rate')
batch_size    = best_hps.get('batch_size')

def build_model():
    text_input = Input(shape=(X_text_train_v2.shape[1],), name='text_input')
    x = Embedding(input_dim=np.max(X_text_train_v2) + 1, output_dim=embed_dim, mask_zero=True)(text_input)
    x = LSTM(lstm_hidden)(x)

    num_input = Input(shape=(X_num_train_v2.shape[1],), name='num_input')
    concat = Concatenate()([x, num_input])
    x = Dense(dense_units)(concat)
    x = ReLU()(x)
    x = Dropout(dropout_rate)(x)
    output = Dense(len(np.unique(y_train_v2)), activation='softmax')(x)

    model = Model(inputs=[text_input, num_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

model = build_model()

checkpoint_path = "best_val_weights.weights.h5"
checkpoint = ModelCheckpoint(
    filepath=checkpoint_path,
    monitor='val_loss',
    save_best_only=True,
    save_weights_only=True,
    verbose=1
)
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

model.fit(
    {'text_input': X_text_train_v2, 'num_input': X_num_train_v2},
    y_train_v2,
    validation_data=({'text_input': X_text_val_v2, 'num_input': X_num_val_v2}, y_val_v2),
    epochs=50,
    batch_size=batch_size,
    callbacks=[early_stop, checkpoint],
    verbose=1
)

train_loss = [
    1.2926, 0.9731, 0.7104, 0.4417, 0.2616, 0.1657,
    0.0993, 0.0628, 0.0498, 0.0425, 0.0279
]

val_loss = [
    1.0526, 1.1505, 1.4685, 1.9628, 2.5961, 2.8171,
    3.1541, 3.4453, 3.6614, 3.9283, 4.4268
]

import matplotlib.pyplot as plt

epochs = range(1, len(train_loss) + 1)

plt.figure(figsize=(8, 6))
plt.plot(epochs, train_loss, 'bo-', label='Training Loss')
plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
plt.title('Training and Validation Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Evaluate F1 - score on Validation Set

model.load_weights(checkpoint_path)

y_val_pred_prob = model.predict([X_text_val_v2, X_num_val_v2], batch_size=batch_size)
y_val_pred = np.argmax(y_val_pred_prob, axis=1)

print("\nClassification Report on Validation Set:")
print(classification_report(y_val_v2, y_val_pred, digits=4))

from sklearn.metrics import classification_report
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping


embed_dim     = best_hps.get('embedding_dim')
lstm_hidden   = best_hps.get('lstm_hidden_units')
dense_units   = best_hps.get('dense_units')
dropout_rate  = best_hps.get('dropout_rate')
learning_rate = best_hps.get('learning_rate')

text_input = Input(shape=(X_text_train_v2.shape[1],), name='text_input')
x = Embedding(input_dim=np.max(X_text_train_v2) + 1, output_dim=embed_dim, mask_zero=True)(text_input)
x = LSTM(lstm_hidden)(x)

num_input = Input(shape=(X_num_train_v2.shape[1],), name='num_input')
concat = Concatenate()([x, num_input])
x = Dense(dense_units)(concat)
x = ReLU()(x)
x = Dropout(dropout_rate)(x)
output = Dense(len(np.unique(y_train_v2)), activation='softmax')(x)

model = Model(inputs=[text_input, num_input], outputs=output)
model.compile(optimizer=Adam(learning_rate),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

history = model.fit(
    {'text_input': X_text_train_v2, 'num_input': X_num_train_v2},
    y_train_v2,
    validation_data=({'text_input': X_text_val_v2, 'num_input': X_num_val_v2}, y_val_v2),
    epochs=1,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

y_val_pred_prob = model.predict([X_text_val_v2, X_num_val_v2])
y_val_pred = np.argmax(y_val_pred_prob, axis=1)

print("\nClassification Report:")
print(classification_report(y_val_v2, y_val_pred, digits=4))

"""### Combine training and validation set to fit the model"""



import numpy as np
import pandas as pd
from sklearn.metrics import classification_report
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, ReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint

# Best hyperparameters
embed_dim     = 640
lstm_hidden   = 32
dense_units   = 96
dropout_rate  = 0.2
learning_rate = 0.001
batch_size    = 16

# Model definition
def build_model():
    text_input = Input(shape=(X_text_train_v2.shape[1],), name='text_input')
    x = Embedding(input_dim=np.max(X_text_train_v2) + 1, output_dim=embed_dim, mask_zero=True)(text_input)
    x = LSTM(lstm_hidden)(x)

    num_input = Input(shape=(X_num_train_v2.shape[1],), name='num_input')
    concat = Concatenate()([x, num_input])
    x = Dense(dense_units)(concat)
    x = ReLU()(x)
    x = Dropout(dropout_rate)(x)
    output = Dense(len(np.unique(y_train_v2)), activation='softmax')(x)

    model = Model(inputs=[text_input, num_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Combine training and validation sets
X_text_all = np.concatenate([X_text_train_v2, X_text_val_v2], axis=0)
X_num_all  = np.concatenate([X_num_train_v2, X_num_val_v2], axis=0)
y_all      = np.concatenate([y_train_v2, y_val_v2], axis=0)

# Build model
final_model = build_model()

# Setup checkpoint saving
checkpoint_path = "/content/drive/MyDrive/Group03QBUS6850_best_2025S1.h5"
checkpoint = ModelCheckpoint(
    filepath=checkpoint_path,
    monitor='val_loss',
    save_best_only=True,
    save_weights_only=False,
    verbose=1
)

# Train model
final_model.fit(
    {'text_input': X_text_all, 'num_input': X_num_all},
    y_all,
    epochs=1,
    batch_size=batch_size,
    validation_split=0.1,
    callbacks=[checkpoint],
    verbose=1
)

# Predict on test set
y_test_pred_prob = final_model.predict([X_text_test_v2, X_num_test_v2], batch_size=batch_size)
y_test_pred = np.argmax(y_test_pred_prob, axis=1)

# Print classification report
labels_6 = ['1', '3', '5', '6', '8', '10']
target_names = labels_6
print("\nClassification Report on Test Set:")
print(classification_report(y_test_v2, y_test_pred, target_names=target_names, digits=4))

# Save predictions to CSV
review_ids = df_test_scaled_v2["ReviewID_Train"].values
label_map = {0: 1, 1: 3, 2: 5, 3: 6, 4: 8, 5: 10}
y_test_rating = [label_map[p] for p in y_test_pred]

submission_df = pd.DataFrame({
    'ReviewID_Train': review_ids,
    'Rating': y_test_rating
})

csv_path = "/content/drive/MyDrive/Group10QBUS6850_2025S1.csv"
submission_df.to_csv(csv_path, index=False)
print(f" Prediction file saved to: {csv_path}")
print(f" Model file saved to: {checkpoint_path}")

# Save best hyperparameters to JSON
import json

best_hparams = {
    'embedding_dim': 640
    'lstm_hidden_units': lstm_hidden,
    'dense_units': dense_units,
    'dropout_rate': dropout_rate,
    'learning_rate': learning_rate,
    'batch_size': batch_size
}

with open("/content/drive/MyDrive/Group03QBUS6850_best_hparams_2025S1.json", 'w') as f:
    json.dump(best_hparams, f, indent=2)



import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report

# --------------- Hyperparameters -------------------
vocab_size     = 6400
embed_dim      = 640
lstm_hidden    = 32
dense_units    = 96
dropout_rate   = 0.2
num_classes    = 6
batch_size     = 16
num_epochs     = 10
learning_rate  = 1e-3

# --------------- 1. Load Data -------------------
X_text_train = X_text_train_v2
X_text_val   = X_text_val_v2
X_num_train  = X_num_train_v2
X_num_val    = X_num_val_v2
y_train      = y_train_v2
y_val        = y_val_v2

X_text_test  = X_text_test_v2
X_num_test   = X_num_test_v2
y_test       = y_test_v2

# Combine train + val
X_text_all = np.concatenate([X_text_train, X_text_val], axis=0)
X_num_all  = np.concatenate([X_num_train, X_num_val], axis=0)
y_all      = np.concatenate([y_train, y_val], axis=0)

# Tensor conversion
X_text_all = torch.tensor(X_text_all, dtype=torch.long)
X_num_all  = torch.tensor(X_num_all, dtype=torch.float32)
y_all      = torch.tensor(y_all, dtype=torch.long)

X_text_test = torch.tensor(X_text_test, dtype=torch.long)
X_num_test  = torch.tensor(X_num_test, dtype=torch.float32)
y_test      = torch.tensor(y_test, dtype=torch.long)

train_dataset = TensorDataset(X_text_all, X_num_all, y_all)
test_dataset  = TensorDataset(X_text_test, X_num_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# --------------- 2. Define Model -------------------
class LSTMWithNumeric(nn.Module):
    def __init__(self):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, lstm_hidden, batch_first=True)
        self.fc1 = nn.Linear(lstm_hidden + X_num_all.shape[1], dense_units)
        self.dropout = nn.Dropout(dropout_rate)
        self.output = nn.Linear(dense_units, num_classes)

    def forward(self, x_text, x_num):
        x = self.embedding(x_text)
        _, (hn, _) = self.lstm(x)
        hn = hn.squeeze(0)
        x = torch.cat([hn, x_num], dim=1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return self.output(x)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMWithNumeric().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()

# --------------- 3. Training -------------------
model.train()
for epoch in range(num_epochs):
    for x_text, x_num, y in train_loader:
        x_text, x_num, y = x_text.to(device), x_num.to(device), y.to(device)

        optimizer.zero_grad()
        logits = model(x_text, x_num)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}")

# --------------- 4. Save Model (.pt) -------------------
torch.save(model.state_dict(), "Group03QBUS6850_best_2025S1.pt")

# --------------- 5. Prediction -------------------
model.eval()
all_preds = []
with torch.no_grad():
    for x_text, x_num, _ in test_loader:
        x_text, x_num = x_text.to(device), x_num.to(device)
        logits = model(x_text, x_num)
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())

# --------------- 6. Evaluation -------------------
label_map = {0: 1, 1: 3, 2: 5, 3: 6, 4: 8, 5: 10}
y_test_label = [label_map[p] for p in all_preds]

print("\nClassification Report on Test Set:")
print(classification_report(y_test.numpy(), all_preds, target_names=['1', '3', '5', '6', '8', '10']))

# --------------- 7. Save Submission -------------------
review_ids = pd.read_csv("reviews_challenge.csv")["ReviewID"]  # Ensure match
submission = pd.DataFrame({
    "ReviewID_Test": review_ids,
    "Rating": y_test_label
})
submission.to_csv("Group03QBUS6850_2025S1.csv", index=False)
print("‚úÖ Prediction CSV and model .pt file saved.")



"""### Save the optimsed parameters in Pytorch"""

torch.save({
'model_state_dict': model.state_dict(),
'hyperparams': {
'embed_dim': 640,
'lstm_hidden': 32,
'dense_units': 96,
'dropout_rate': 0.2,
'batch_size': 16,
'learning_rate': 0.001
}
}, '/content/drive/MyDrive/Group10QBUS6850_best_2025S1.pt')

torch.save(model.state_dict(), '/content/drive/MyDrive/Group10QBUS6850_best_2025S1.pt')

y_all.min()

np.unique(y_all)

y_train.dtype

print("X_text_test:", X_text_test.shape)
print("X_num_test:", X_num_test.shape)
print("y_test:", y_test.shape)

"""# Prediction on Test set

## pre-processing for test dataset
"""

print(df_test_original.isnull().sum())

df_test_original['Strengths'] = df_test_original['Strengths'].fillna('Not provided')
df_test_original['Weaknesses'] = df_test_original['Weaknesses'].fillna('Not provided')

# Define your preprocessing function

# Precompile URL regex
url_pattern = re.compile(r'https?://\S+|www\.\S+')

def clean_text(text):
    text = text.lower() # Convert to lowercase
    text = re.sub(r'[^a-z\s]', ' ', text)  # Remove numbers, punctuation, special chars
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    return text

# Columns to process
text_columns = ['title', 'abstract', 'Summary', 'Strengths', 'Weaknesses']

# Apply preprocessing directly (since no NaNs exist)
for col in ['title', 'abstract', 'Summary', 'Strengths', 'Weaknesses']:
    df_test_original[col] = df_test_original[col].apply(clean_text)

df_test_original.head()

# Initialize basic English tokenizer
tokenizer = get_tokenizer("basic_english")

# Tokenize each text column
for col in ['title', 'abstract', 'Summary', 'Strengths', 'Weaknesses']:
    token_col = col + '_tokens'
    df_test_original[token_col] = df_test_original[col].apply(tokenizer)

# Load English stopwords
stop_words = set(stopwords.words('english'))

# Remove stopwords from tokenized columns
for col in ['title_tokens', 'abstract_tokens', 'Summary_tokens', 'Strengths_tokens', 'Weaknesses_tokens']:
    df_test_original[col] = df_test_original[col].apply(lambda tokens: [token for token in tokens if token not in stop_words])

#import nltk
from nltk.corpus import wordnet
from nltk import pos_tag

# Download required NLTK resources
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to convert POS tag to WordNet format
def get_wordnet_pos(word):
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {
        'J': wordnet.ADJ,
        'N': wordnet.NOUN,
        'V': wordnet.VERB,
        'R': wordnet.ADV
    }
    return tag_dict.get(tag, wordnet.NOUN)

# Function to lemmatize a list of tokens using POS
def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]



for col in ['title_tokens', 'abstract_tokens', 'Summary_tokens', 'Strengths_tokens', 'Weaknesses_tokens']:
    df_test_original[col] = df_test_original[col].apply(lemmatize_tokens)

df_test_original['combined_tokens'] = df_test_original['Summary_tokens'] + df_test_original['Strengths_tokens'] + df_test_original['Weaknesses_tokens']

# Step 3: Feature Scaling (X)
# ----------------------------
from sklearn.preprocessing import MinMaxScaler

# Columns to scale
numcl = ['Soundness', 'Presentation', 'Contribution', 'Confidence']

# Initialize MinMaxScaler (default scales to [0, 1])
scaler = MinMaxScaler()

# Fit scaler on training data only
df_test_scaled = df_test_original.copy()
df_test_scaled[numcl] = scaler.fit_transform(df_test_scaled[numcl])

text_colum =['combined_tokens']
numcl = ['Soundness', 'Presentation', 'Contribution', 'Confidence']

from tensorflow.keras.preprocessing.text import Tokenizer
# Initialize and fit tokenizer on the scaled training data (v2)
tokenizer_v2 = Tokenizer()
tokenizer_v2.fit_on_texts(df_test_scaled['combined_tokens'])


# Convert text to sequences using the same tokenizer
X_text_scaled = tokenizer_v2.texts_to_sequences(df_test_scaled['combined_tokens'])







"""## Prediction"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd


# -----------------------------
X_text_seq = tokenizer_v2.texts_to_sequences(df_test_scaled['combined_tokens'])
X_text_pad = pad_sequences(X_text_seq, padding='post', truncating='post', maxlen=512)


numcl = ['Soundness', 'Presentation', 'Contribution', 'Confidence']
X_num = df_test_scaled[numcl].values.astype(np.float32)

X_text_tensor = torch.tensor(X_text_pad, dtype=torch.long)
X_num_tensor = torch.tensor(X_num, dtype=torch.float32)

test_dataset = TensorDataset(X_text_tensor, X_num_tensor)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

embed_dim = 640
lstm_hidden = 32
dense_units = 96
dropout_rate = 0.2
num_classes = 6
vocab_size = 51508
class LSTMWithNumeric(nn.Module):
    def __init__(self):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, lstm_hidden, batch_first=True)
        self.fc1 = nn.Linear(lstm_hidden + X_num_tensor.shape[1], dense_units)
        self.dropout = nn.Dropout(dropout_rate)
        self.output = nn.Linear(dense_units, num_classes)

    def forward(self, x_text, x_num):
        x = self.embedding(x_text)
        x, _ = self.lstm(x)
        x = x[:, -1, :]
        x = torch.cat([x, x_num], dim=1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return self.output(x)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMWithNumeric().to(device)
checkpoint = torch.load("/content/drive/MyDrive/Group10QBUS6850_best_2025S1.pt", map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

all_preds = []
with torch.no_grad():
    for x_text_batch, x_num_batch in test_loader:
        x_text_batch = x_text_batch.to(device)
        x_num_batch = x_num_batch.to(device)
        logits = model(x_text_batch, x_num_batch)
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())

inv_map = {0: 1, 1: 3, 2: 5, 3: 6, 4: 8, 5: 10}
final_preds = [inv_map[p] for p in all_preds]
df_submission = pd.DataFrame({
    'ReviewID_Test': df_test_scaled['ReviewID_Test'].values,
    'prediction': final_preds
})

df_submission.to_csv("/content/drive/MyDrive/Group10QBUS6850_submission_predictions.csv", index=False)

print("‚úÖ Saved to Group10QBUS6850_submission_predictions.csv with only ReviewID_Train and prediction.")




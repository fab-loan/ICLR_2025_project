# -*- coding: utf-8 -*-
"""Task B_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1alSY0Gi_3akXKf2Qrjq93NClEfuFtqGB
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import libraries and dataset"""

pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118

pip install torchtext

# Import all necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans

# For Data cleaning and Pre-processing
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.util import ngrams
from sklearn.feature_extraction.text import TfidfVectorizer
from torchtext.data.utils import get_tokenizer
from nltk.corpus import wordnet
from nltk import pos_tag


# The submission dataset is called df_sub in this report

# Replace by own path
df_sub = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Submissions.csv')
df_sub.head(2)

"""# General Dataset Information"""

# Check for missing values in the training, validation, and test data
print("Missing values in submissions data:")
print(df_sub.isnull().sum())

# Check basic information about the datasets (data types, number of n/a)
print("\nSubmissions data info:")
print(df_sub.info())

# Check for duplicate rows (excluding ID columns)
print("\nDuplicate rows:",
      df_sub.duplicated(subset=[col for col in df_sub.columns if col != 'reviewID_train']).sum())

# Summary statistics to understand the distribution of numerical columns
print("\nSubmissions data summary statistics:")
print(df_sub.describe())

"""# Text Pre-Processing

1. Text Cleaning

* convert to lowercase
* remove URLs
* remove punctuation, other characters
* remove digits/ numbers
"""

# Drop TLDR column
df_sub.drop(columns=['tldr'], inplace=True)

# Check if some missing values are left
print("Missing values in training data:")
print(df_sub.isnull().sum())

# Setup for Data cleaning and pre-processing
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Initialize tokenizer
tokenizer = get_tokenizer("basic_english")

# URL‐removal regex
url_pattern = re.compile(r'https?://\S+|www\.\S+')

def clean_text(text):
    text = text.lower()                            # lowercase
    text = re.sub(url_pattern, ' ', text)          # strip URLs
    text = re.sub(r'[^a-z\s]', ' ', text)          # remove punctuation/numbers
    text = re.sub(r'\s+', ' ', text).strip()       # collapse spaces
    return text

# Columns to process
text_columns = ['title', 'abstract', 'keywords']

# Apply preprocessing directly since there no n/a
for col in ['title', 'abstract', 'keywords']:
    df_sub[col] = df_sub[col].apply(clean_text)

"""2. Tokenisation"""

# Initialize basic English tokenizer
tokenizer = get_tokenizer("basic_english")

# Tokenize each text column
for col in ['title', 'abstract', 'keywords']:
    token_col = col + '_tokens'
    df_sub[token_col] = df_sub[col].apply(tokenizer)

df_sub.head(2)

"""3. Stop-Word Removal --> NLTK"""

# Load English stopwords
stop_words = set(stopwords.words('english'))

# Remove stopwords from tokenized columns
for col in ['title_tokens', 'abstract_tokens', 'keywords_tokens']:
    df_sub[col] = df_sub[col].apply(lambda tokens: [token for token in tokens if token not in stop_words])

"""4. Lemmatization --> WordNetLemmatizer & POS_tag"""

# Download required NLTK resources
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to convert POS tag to WordNet format
def get_wordnet_pos(word):
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {
        'J': wordnet.ADJ,
        'N': wordnet.NOUN,
        'V': wordnet.VERB,
        'R': wordnet.ADV
    }
    return tag_dict.get(tag, wordnet.NOUN)

# Function to lemmatize a list of tokens using POS
def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]

for col in ['title_tokens', 'abstract_tokens', 'keywords_tokens']:
    df_sub[col] = df_sub[col].apply(lemmatize_tokens)

df_sub_processed = df_sub.to_csv("/content/drive/MyDrive/ICLR2025 Files/df_sub_processed.csv", index=False)

df_sub_processed= pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/df_sub_processed.csv')
df_sub_processed.head(2)

"""Combining tokens and columns"""

# Combine strings into one tokenized string
df_sub_processed['combined_text'] = (
    df_sub_processed['title_tokens'].astype(str) + ' ' +
    df_sub_processed['abstract_tokens'].astype(str) + ' ' +
    df_sub_processed['keywords_tokens'].astype(str)
)

df_sub_processed.head(2)

"""# K-Means

# BoW + K-MEANS
"""

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(max_features=1000)

# If the column contains lists, we convert it back to strings
df_sub_processed['combined_text'] = df_sub_processed['combined_text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)

# Transform the combined text into BoW vectors
X_bow = vectorizer.fit_transform(df_sub_processed['combined_text'])

# Display the shape of the matrix
print(f"Bag of Words Matrix Shape: {X_bow.shape}")

from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Setup CountVectorizer for Bag of Words and preprocess text
vectorizer = CountVectorizer(stop_words='english', max_features=500, ngram_range=(1, 3))

# Use combined_text for vectorization
text_data = df_sub_processed['combined_text'].fillna('')
X_bow = vectorizer.fit_transform(text_data)

"""Elbow method to determine optimal number of clusters"""

wcss = []

for i in range(1, 16):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(X_bow)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method
plt.figure(figsize=(8, 5))
plt.plot(range(1, 16), wcss, marker='o')
plt.title('BoW + K-Means - Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.grid(True)
plt.show()

"""Silhouette Score

Pick the k that maximizes this score (often around where the elbow was)
"""

from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans

sil_scores = []

for i in range(2, 16):
    kmeans = KMeans(
        n_clusters=i,
        init='k-means++',
        max_iter=300,
        n_init=10,
        random_state=42
    )
    labels = kmeans.fit_predict(X_bow)
    score = silhouette_score(X_bow, labels, metric='euclidean')
    sil_scores.append(score)
    print(f"k={i:2d} → silhouette score = {score:.4f}")

# Plotting the Silhouette Scores
plt.figure(figsize=(8, 5))
plt.plot(range(2, 16), sil_scores, marker='o')
plt.title('Silhouette Scores for Different Numbers of Clusters')
plt.xlabel('Number of Clusters k')
plt.ylabel('Silhouette Score')
plt.xticks(range(2, 16))
plt.grid(True)
plt.show()

"""Interpretation

If you want the most clearly separated clusters (highest silhouette), choose k = 2.

If you need a bit more granularity while still respecting the elbow, k = 3 is a great compromise.
"""

# Apply t-SNE for dimensionality reduction to 2D space
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X_bow.toarray())

X_scaled = StandardScaler().fit_transform(X_tsne)

kmeans = KMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(X_scaled)

df_sub_processed['tsne_1'] = X_scaled[:, 0]
df_sub_processed['tsne_2'] = X_scaled[:, 1]
df_sub_processed['cluster'] = labels

# Plot the t-SNE clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_sub_processed, x='tsne_1', y='tsne_2', hue='cluster', palette='tab10', s=40, alpha=0.7)

plt.title("t-SNE Visualisation Bag of Words + K-Means")
plt.xlabel("t-SNE Dimension 1")
plt.ylabel("t-SNE Dimension 2")

plt.legend(title="Cluster", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""Interpretaton:

Randomly allocated to clusted. It is bad because it is too perfect, and there is more than 2 clusters.

# TF-IDF + K-MEANS
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
X_tfidf = tfidf_vectorizer.fit_transform(df_sub_processed['combined_text'])

# Apply Elbow Method to find the optimal number of clusters
inertia = []
cluster_range = range(1, 16)

for k in cluster_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_tfidf)
    inertia.append(kmeans.inertia_)

# Plot the Elbow curve
plt.figure(figsize=(8, 6))
plt.plot(cluster_range, inertia, marker='o', linestyle='-', color='b')
plt.title('TF-IDF + K-Means - Elbow Method for Optimal Number of Clusters', fontsize=16)
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.xticks(cluster_range)
plt.grid(True)
plt.show()

"""Silouhette Score"""

# Vectorize with TF-IDF
tfidf_vectorizer = TfidfVectorizer(
    stop_words='english',
    max_features=1000
)
X_tfidf = tfidf_vectorizer.fit_transform(
    df_sub_processed['combined_text']
)

# Compute silhouette scores
sil_scores = []
cluster_range = range(2, 16)

for k in cluster_range:
    km = KMeans(
        n_clusters=k,
        init='k-means++',
        n_init=10,
        max_iter=300,
        random_state=42
    )
    labels = km.fit_predict(X_tfidf)
    score = silhouette_score(X_tfidf, labels, metric='euclidean')
    sil_scores.append(score)
    print(f"k={k:2d} → silhouette score = {score:.4f}")

# Plot
plt.figure(figsize=(8, 6))
plt.plot(cluster_range, sil_scores, marker='o', linestyle='-')
plt.title('TF-IDF + K-Means – Silhouette Scores for Different k', fontsize=14)
plt.xlabel('Number of Clusters k')
plt.ylabel('Silhouette Score')
plt.xticks(cluster_range)
plt.grid(True)
plt.show()

"""From 1 to 4 clusters, there is a significant drop in inertia (Within-Cluster Sum of Squares).
The curve starts to flatten out around 4 or 5 clusters.
This "elbow" point suggests that the major reduction in inertia has already been achieved.


After 5 clusters, the curve smoothens, indicating diminishing returns.
The optimal number of clusters is likely around 4 or 5, as this is where the "elbow" effect is most visible.

Choosing 4 clusters would prioritize simplicity, while 5 clusters would capture slightly more detail.
We pick 5 clusters for more precision

Plotting
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE

# Step 1: TF-IDF Vectorization
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2))
X_tfidf = vectorizer.fit_transform(df_sub_processed['combined_text'])

# Step 2: Convert to Dense Array
if X_tfidf.shape[0] * X_tfidf.shape[1] < 1e7:
    X_tfidf_dense = X_tfidf.toarray()
else:
    print("Large data detected, switching to incremental loading...")
    X_tfidf_dense = np.array_split(X_tfidf.toarray(), 10)
    X_tfidf_dense = np.vstack(X_tfidf_dense)

# Step 3: Apply KMeans
kmeans = KMeans(n_clusters=8, random_state=42)
df_sub_processed['cluster_kmeans'] = kmeans.fit_predict(X_tfidf_dense)

# Step 4: Apply t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_tsne_kmeans = tsne.fit_transform(X_tfidf_dense)

# Step 5: Add t-SNE coordinates to DataFrame
df_sub_processed['tsne_1'] = X_tsne_kmeans[:, 0]
df_sub_processed['tsne_2'] = X_tsne_kmeans[:, 1]

# Step 6: Plot clusters
plt.figure(figsize=(10, 8))
sns.scatterplot(
    data=df_sub_processed,
    x='tsne_1',
    y='tsne_2',
    hue='cluster_kmeans',
    palette='tab10',
    s=50,
    alpha=0.7
)
plt.title('t-SNE Visualization of TF-IDF + KMeans Clusters', fontsize=16)
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.legend(title="Cluster", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation

# Prepare custom stopwords
custom_stopwords = {
    'study','research','paper','results','data','dataset','method',
    'analysis','conclusion','approach','finding','model','effect',
    'based','use','using','present','show','provide','feature','task'
}
all_stopwords = list(text.ENGLISH_STOP_WORDS.union(custom_stopwords))

# TF-IDF on your combined text
tfidf_vectorizer = TfidfVectorizer(
    stop_words=all_stopwords,
    max_features=1000
)
X_tfidf = tfidf_vectorizer.fit_transform(
    df_sub_processed['combined_text']
)
feature_names = tfidf_vectorizer.get_feature_names_out()

# KMeans - 8 clusters
n_clusters = 8
kmeans = KMeans(
    n_clusters=n_clusters,
    init='k-means++',
    n_init=10,
    max_iter=300,
    random_state=42
)
df_sub_processed['cluster'] = kmeans.fit_predict(X_tfidf)

# LDA within each cluster to pull its main topic ———
cluster_topics = {}
for cluster_num in range(n_clusters):
    print(f"\n--- Cluster {cluster_num} — { (df_sub_processed['cluster']==cluster_num).sum() } docs")

    # Select TF-IDF rows for this cluster
    idx = df_sub_processed.index[df_sub_processed['cluster'] == cluster_num]
    X_cluster = X_tfidf[idx]
    if X_cluster.shape[0] == 0:
        print("  (empty cluster)")
        continue

    # Fit a 1-topic LDA
    lda = LatentDirichletAllocation(
        n_components=1,
        random_state=42,
        learning_method='batch'
    )
    lda.fit(X_cluster)

    # Get top 10 words
    topic_dist = lda.components_[0]
    top_idxs = topic_dist.argsort()[::-1][:10]
    keywords = [feature_names[i] for i in top_idxs]

    cluster_topics[cluster_num] = keywords
    print("  Top keywords:", ", ".join(keywords))

topics_df = (
    pd.DataFrame.from_dict(cluster_topics, orient='index')
      .reset_index()
      .rename(columns={'index': 'cluster'})
)
topics_df.columns = ['cluster'] + [f'keyword_{i+1}' for i in range(10)]
print("\nSummary of cluster topics:")
print(topics_df)

from sklearn.decomposition import LatentDirichletAllocation

# Parameters
num_clusters = 8
top_n        = 10
cols         = 4
rows         = 2

fig, axes = plt.subplots(rows, cols, figsize=(20, 8))
axes = axes.flatten()

for i, cluster_num in enumerate(range(num_clusters)):
    ax = axes[i]

    # Re-extract this cluster’s TF-IDF slice
    idx       = df_sub_processed.index[df_sub_processed['cluster'] == cluster_num]
    X_cluster = X_tfidf[idx]
    if X_cluster.shape[0] == 0:
        ax.set_visible(False)
        continue

    # Fit a single‐topic LDA
    lda = LatentDirichletAllocation(n_components=1, random_state=42, learning_method='batch')
    lda.fit(X_cluster)

    # Pull top_n words + weights
    weights     = lda.components_[0]
    top_indices = weights.argsort()[::-1][:top_n]
    words       = [feature_names[j] for j in top_indices][::-1]
    vals        = weights[top_indices][::-1]

    # Draw horizontal bar chart with highest at the top
    ax.barh(words, vals, color='skyblue')
    ax.set_title(f"Cluster {cluster_num}", fontsize=12)
    ax.set_xlabel("LDA Weight")
    ax.invert_yaxis()

# Remove any leftover subplots
for j in range(num_clusters, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.suptitle("Top 10 LDA Keywords per Cluster (8-Cluster Model)", fontsize=16, y=1.02)
plt.show()

"""# DBSCAN

# Bow + DBSCAN

No need to determine the optimal number of
"""

from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import DBSCAN

# Bag-of-Words on your combined_text
vectorizer = CountVectorizer(
    stop_words=all_stopwords,
    max_features=1000,
    ngram_range=(1, 2)
)
X_bow = vectorizer.fit_transform(df_sub_processed['combined_text'])

# Safely convert to dense array
if X_bow.shape[0] * X_bow.shape[1] < 1e7:
    X_dense = X_bow.toarray()
else:
    # break into batches to avoid MemoryError
    batches = np.array_split(X_bow.toarray(), 10)
    X_dense = np.vstack(batches)

# DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5, metric='cosine')
labels = dbscan.fit_predict(X_dense)
df_sub_processed['cluster_dbscan'] = labels

#t-SNE for 2D projection
tsne = TSNE(
    n_components=2,
    random_state=42,
    learning_rate='auto',
    init='random'
)
X_tsne = tsne.fit_transform(X_dense)
df_sub_processed['tsne_1'] = X_tsne[:, 0]
df_sub_processed['tsne_2'] = X_tsne[:, 1]

plt.figure(figsize=(10, 8))
sns.scatterplot(
    data=df_sub_processed,
    x='tsne_1', y='tsne_2',
    hue='cluster_dbscan',
    palette='tab10',
    s=50, alpha=0.7,
    legend='full'
)

plt.title('t-SNE Visualization of BoW + DBSCAN Clusters', fontsize=16)
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.legend(title='DBSCAN Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""# TF-IDF + DBSCAN"""

from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE


#TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(
    stop_words=all_stopwords,
    max_features=1000,
    ngram_range=(1, 2)
)
X_tfidf = tfidf_vectorizer.fit_transform(
    df_sub_processed['combined_text']
)

#DBSCAN directly on sparse TF-IDF
dbscan = DBSCAN(
    eps=0.5,
    min_samples=5,
    metric='cosine',
    n_jobs=-1
)
df_sub_processed['cluster_dbscan'] = dbscan.fit_predict(X_tfidf)

#Batch-wise sparse
batch_size = 5000
batches = [
    X_tfidf[i : i + batch_size].toarray()
    for i in range(0, X_tfidf.shape[0], batch_size)
]
X_dense = np.vstack(batches)

# t-SNE projection
tsne = TSNE(
    n_components=2,
    random_state=42,
    learning_rate='auto',
    init='random'
)
X_tsne = tsne.fit_transform(X_dense)

df_sub_processed['tsne_1'] = X_tsne[:, 0]
df_sub_processed['tsne_2'] = X_tsne[:, 1]

plt.figure(figsize=(10, 8))
sns.scatterplot(
    data=df_sub_processed,
    x='tsne_1', y='tsne_2',
    hue='cluster_dbscan',
    palette='tab10',
    s=50, alpha=0.7,
    legend='full'
)

plt.title('t-SNE Visualization of TF-IDF + DBSCAN Clusters', fontsize=16)
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.legend(title='DBSCAN Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""# GMMS

# Bow + GMMs
"""

from sklearn.mixture import GaussianMixture

#Bag‐of‐Words
vec = CountVectorizer(max_features=1000, ngram_range=(1,2))
X_sparse = vec.fit_transform(df_sub_processed['combined_text'])

#Dense conversion
if X_sparse.shape[0] * X_sparse.shape[1] < 1e7:
    X = X_sparse.toarray()
else:
    chunks = [X_sparse[i:i+5000].toarray()
              for i in range(0, X_sparse.shape[0], 5000)]
    X = np.vstack(chunks)

#GMM clustering (k=8)
labels = GaussianMixture(n_components=8, random_state=42).fit_predict(X)
df_sub_processed['cluster_gmm'] = labels

#t-SNE to 2D
proj = TSNE(n_components=2, random_state=42, learning_rate='auto', init='random') \
       .fit_transform(X)
df_sub_processed[['tsne_1', 'tsne_2']] = proj

plt.figure(figsize=(10,8))
sns.scatterplot(
    x='tsne_1', y='tsne_2',
    hue='cluster_gmm', palette='tab10',
    data=df_sub_processed, s=50, alpha=0.7
)
plt.title('t-SNE visualization of BoW + GMM')
plt.legend(title='Cluster', bbox_to_anchor=(1.05,1), loc='upper left')
plt.tight_layout()
plt.show()

"""# TF-IDF + GMMs"""

# TF-IDF
X = TfidfVectorizer(max_features=1000, stop_words='english') \
    .fit_transform(df_sub_processed['combined_text']) \
    .toarray()

# GMM clustering
df_sub_processed['cluster'] = GaussianMixture(
    n_components=8,
    random_state=42
).fit_predict(X)

# t-SNE projection
proj = TSNE(
    n_components=2,
    random_state=42,
    learning_rate='auto',
    init='random'
).fit_transform(X)
df_sub_processed[['tsne_1', 'tsne_2']] = proj

plt.figure(figsize=(10, 8))
sns.scatterplot(
    data=df_sub_processed,
    x='tsne_1', y='tsne_2',
    hue='cluster', palette='tab10', s=50, alpha=0.7
)
plt.title('t-SNE Visulisation of TF-IDF + GMM')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()
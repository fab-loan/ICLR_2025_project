# -*- coding: utf-8 -*-
"""TASK D - BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zj_CBZ2hKtqvplQomlf7j2jMYu7iRYHv
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import re

from sklearn.preprocessing import StandardScaler

import os
import re
import string
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm # Progress Bar
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.utils import class_weight
import transformers
from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
import warnings
from transformers import logging as hf_logging
hf_logging.set_verbosity_error() # Hidding Huggingface Warnings
warnings.filterwarnings("ignore")

#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#print(f'Using device: {device}')

"""# Read Files + Impute Missing Values"""

df_test_original = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Rating_Test.csv')
df_test_original.head(5)

df_train_original = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Rating_Train.csv')
df_train_original .head(5)

"""Missing Values


*   Found in 'stengths' & 'weakness'
*   Impute with a Placeholder --> Since it's text data, you can replace missing values with a placeholder string such as 'No data available' or 'Not provided'. This way, you can retain all rows without losing any data.

Duplicate Values

* One paper is check multiple times, it has different scores and different strenghts & weaknesses

"""

# Check for missing values in the training, validation, and test data
print("Missing values in training data:")
print(df_train_original.isnull().sum())

# Check basic information about the datasets (e.g., data types, number of non-null entries)
print("\nTraining data info:")
print(df_train_original.info())

# Check for duplicate rows (excluding ID columns)
print("\nDuplicate rows in training data (excluding review_ID_train):",
      df_train_original.duplicated(subset=[col for col in df_train_original.columns if col != 'reviewID_train']).sum())

# Summary statistics to understand the distribution of numerical columns
print("\nTraining data summary statistics:")
print(df_train_original.describe())

# Impute missing values with 'Not provided' for Strengths and Weaknesses and Summary in ICLR_train, ICLR_vali, and ICLR_test
df_train_original['Strengths'] = df_train_original['Strengths'].fillna('Not provided')
df_train_original['Weaknesses'] = df_train_original['Weaknesses'].fillna('Not provided')
df_train_original['Summary'] = df_train_original['Summary'].fillna('Not provided')

# Check whether missing values are gone
print("Missing values in training data:")
print(df_train_original[['Strengths', 'Weaknesses','Summary']].isnull().sum())

numeric_cols = ['Soundness', 'Presentation', 'Contribution', 'Confidence']

for col in numeric_cols:
    min_val = df_train_original[col].min()
    max_val = df_train_original[col].max()
    print(f"{col}: Min = {min_val}, Max = {max_val}")

import matplotlib.pyplot as plt
import seaborn as sns

# Define numeric columns
numeric_cols = ['Soundness', 'Presentation', 'Contribution', 'Confidence']

# Melt into long format for seaborn
numeric_data = df_train_original[numeric_cols]
numeric_data_melted = numeric_data.melt(var_name='Metric', value_name='Score')

# Plot boxplots
plt.figure(figsize=(8, 6))
sns.boxplot(data=numeric_data_melted, x='Metric', y='Score')
plt.title('Outlier Detection in Reviewer Scores', fontsize=14, fontweight='bold')
plt.xlabel('Review Metric')
plt.ylabel('Score')
plt.tight_layout()
plt.show()

# Print outliers for each numeric column
for col in numeric_cols:
    Q1 = df_train_original[col].quantile(0.25)
    Q3 = df_train_original[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df_train_original[(df_train_original[col] < lower_bound) | (df_train_original[col] > upper_bound)]

    print(f"\n🔍 Outliers in '{col}' (outside {lower_bound:.2f} to {upper_bound:.2f}):")
    print(outliers[[col]])

"""# Train-Vali-Split

stratify split to account for imbalanced classes
"""

# Step 1: 70% train, 30% temp (stratified)
df_train, df_temp = train_test_split(
    df_train_processed,
    test_size=0.30,
    stratify=df_train_processed['Rating'],
    random_state=42
)

# Step 2: Split temp into 20% validation and 10% test
# Use stratify again to preserve distribution
df_vali, df_test = train_test_split(
    df_temp,
    test_size=0.33,  # 33% of 30% ≈ 10% of total
    stratify=df_temp['Rating'],
    random_state=42
)

print("Train shape:", df_train.shape)
print("Validation shape:", df_vali.shape)
print("Test shape:", df_test.shape)

"""# SHAP Numerical Values"""

import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier

features = ['Soundness','Presentation','Contribution','Confidence']
X = df_train_original[features].astype(float)   # should now be (n_samples, 4)
y = df_train_original['Rating'].astype(int)

print("X.shape →", X.shape)
print("X.columns →", X.columns.tolist())

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

explainer   = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

for i, cls in enumerate(model.classes_):
    print("Class", cls, "→", shap_values[i].shape)

# 1) Prep
features = ['Soundness','Presentation','Contribution','Confidence']
target   = 'Rating'
classes_to_plot = [1,3,5,6,8,10]

X = df_train_original[features].astype(float)
y = df_train_original[target].astype(int)

# 2) Fit
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

# 3) SHAP
explainer   = shap.TreeExplainer(model)
shap_vals3d = explainer.shap_values(X)  # shape = (N_samples, 4, 6)

print("shap_vals3d.shape:", shap_vals3d.shape)
# -> (28048, 4, 6)

fig, axes = plt.subplots(3,2,figsize=(14,12))
axes = axes.flatten()

for ax, cls in zip(axes, classes_to_plot):
    # find index of this class in model.classes_
    cls_idx = list(model.classes_).index(cls)

    # slice out a (N_samples x 4) array for this class
    arr = shap_vals3d[:,:,cls_idx]                # -> (N,4)

    # compute mean absolute impact per feature
    mean_abs = np.abs(arr).mean(axis=0)           # -> (4,)

    # turn into a Series so we can sort by feature name
    imp = pd.Series(mean_abs, index=features).sort_values()

    # plot
    ax.barh(imp.index, imp.values, color='dodgerblue')
    ax.set_title(f"Class {cls}")
    ax.set_xlabel("mean(|SHAP value|)")

plt.tight_layout()
plt.savefig("shap_rating_bars_rf.pdf", dpi=300)
plt.show()

"""# Naive Bayes"""

# # Label encoding for Rating
# rating_to_label = {1: 0, 3: 1, 5: 2, 6: 3, 8: 4, 10: 5}
# label_to_rating = {v: k for k, v in rating_to_label.items()}
# df_train_original['label'] = df_train_original['Rating'].map(rating_to_label)

# 'title', 'abstract', 'Summary', 'Strengths', 'Weaknesses'
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report

text_columns = ['title', 'abstract','Summary', 'Strengths', 'Weaknesses']

for col in text_columns:
    df_train[col] = df_train[col].fillna('')
    df_vali[col] = df_vali[col].fillna('')
    df_test[col] = df_test[col].fillna('')

df_train['text'] = df_train[text_columns].agg(' '.join, axis=1)
df_vali['text'] = df_vali[text_columns].agg(' '.join, axis=1)
df_test['text'] = df_test[text_columns].agg(' '.join, axis=1)

X_train, y_train = df_train['text'], df_train['Rating']
X_val, y_val = df_vali['text'], df_vali['Rating']
X_test, y_test = df_test['text'], df_test['Rating']

nb_pipeline = make_pipeline(
    TfidfVectorizer(max_features=5000, stop_words='english'),
    MultinomialNB()
)

nb_pipeline.fit(X_train, y_train)

print("Validation Set Results:")
print(classification_report(y_val, nb_pipeline.predict(X_val)))

print("Test Set Results:")
print(classification_report(y_test, nb_pipeline.predict(X_test)))

text_columns = ['title', 'abstract','Summary', 'Strengths', 'Weaknesses']
numeric_columns = ['Soundness', 'Presentation', 'Contribution', 'Confidence']

for col in text_columns + numeric_columns:
    df_train[col] = df_train[col].fillna('')
    df_vali[col] = df_vali[col].fillna('')
    df_test[col] = df_test[col].fillna('')

df_train['text'] = df_train[text_columns].agg(' '.join, axis=1)
df_vali['text'] = df_vali[text_columns].agg(' '.join, axis=1)
df_test['text'] = df_test[text_columns].agg(' '.join, axis=1)

X_train = df_train[['text'] + numeric_columns]
X_val = df_vali[['text'] + numeric_columns]
X_test = df_test[['text'] + numeric_columns]

y_train = df_train['Rating']
y_val = df_vali['Rating']
y_test = df_test['Rating']

preprocessor = ColumnTransformer(
    transformers=[
        ('text', TfidfVectorizer(max_features=5000, stop_words='english'), 'text'),
        ('num', MinMaxScaler(), numeric_columns)  # scale numeric 1–5 to [0, 1]
    ]
)

nb_pipeline = make_pipeline(
    preprocessor,
    MultinomialNB()
)

nb_pipeline.fit(X_train, y_train)

print("Validation Set:")
print(classification_report(y_val, nb_pipeline.predict(X_val)))

print("Test Set:")
print(classification_report(y_test, nb_pipeline.predict(X_test)))

# 'title', 'abstract', 'Summary', 'Strengths', 'Weaknesses'
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report

# STEP 1: Combine cleaned text fields into a single 'text' column
text_columns = ['Summary', 'Strengths', 'Weaknesses']

# Replace NaNs with empty strings in those columns
for col in text_columns:
    df_train[col] = df_train[col].fillna('')
    df_vali[col] = df_vali[col].fillna('')
    df_test[col] = df_test[col].fillna('')

df_train['text'] = df_train[text_columns].agg(' '.join, axis=1)
df_vali['text'] = df_vali[text_columns].agg(' '.join, axis=1)
df_test['text'] = df_test[text_columns].agg(' '.join, axis=1)

# STEP 2: Define features (X) and labels (y)
X_train, y_train = df_train['text'], df_train['Rating']
X_val, y_val = df_vali['text'], df_vali['Rating']
X_test, y_test = df_test['text'], df_test['Rating']

# STEP 3: Build pipeline with TF-IDF + Naive Bayes
nb_pipeline = make_pipeline(
    TfidfVectorizer(max_features=5000, stop_words='english'),
    MultinomialNB()
)

# STEP 4: Train the model
nb_pipeline.fit(X_train, y_train)

# STEP 5: Evaluate
print("Validation Set Results:")
print(classification_report(y_val, nb_pipeline.predict(X_val)))

print("Test Set Results:")
print(classification_report(y_test, nb_pipeline.predict(X_test)))

# Define columns
text_columns = ['Summary', 'Strengths', 'Weaknesses']
numeric_columns = ['Soundness', 'Presentation', 'Contribution', 'Confidence']

# Fill NA
for col in text_columns + numeric_columns:
    df_train[col] = df_train[col].fillna('')
    df_vali[col] = df_vali[col].fillna('')
    df_test[col] = df_test[col].fillna('')

# Join text columns
df_train['text'] = df_train[text_columns].agg(' '.join, axis=1)
df_vali['text'] = df_vali[text_columns].agg(' '.join, axis=1)
df_test['text'] = df_test[text_columns].agg(' '.join, axis=1)

# Feature matrix and target
X_train = df_train[['text'] + numeric_columns]
X_val = df_vali[['text'] + numeric_columns]
X_test = df_test[['text'] + numeric_columns]

y_train = df_train['Rating']
y_val = df_vali['Rating']
y_test = df_test['Rating']

# Column transformer to combine text + numeric preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ('text', TfidfVectorizer(max_features=5000, stop_words='english'), 'text'),
        ('num', MinMaxScaler(), numeric_columns)  # scale numeric 1–5 to [0, 1]
    ]
)

# Pipeline
nb_pipeline = make_pipeline(
    preprocessor,
    MultinomialNB()
)

# Train
nb_pipeline.fit(X_train, y_train)

# Evaluate
print("Validation Set:")
print(classification_report(y_val, nb_pipeline.predict(X_val)))

print("Test Set:")
print(classification_report(y_test, nb_pipeline.predict(X_test)))

"""# Task D

https://huggingface.co/distilbert-base-uncased
"""

# !pip show transformers

#import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("GPU available:", tf.config.list_physical_devices('GPU')) #check if using GPU

import os
import re
import string
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm # Progress Bar
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import transformers
from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
import warnings
from transformers import logging as hf_logging
hf_logging.set_verbosity_error() # Hidding Huggingface Warnings
warnings.filterwarnings("ignore")

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

df_train_original.head()

# Label encoding for Rating
rating_to_label = {1: 0, 3: 1, 5: 2, 6: 3, 8: 4, 10: 5}
label_to_rating = {v: k for k, v in rating_to_label.items()}
df_train_original['label'] = df_train_original['Rating'].map(rating_to_label)

"""# Model 1 - Summary, Strengths, Weaknesses, Soundness, Presentation, Contribution, Confidence"""

# Combine relevant text columns
def combine_text(row):
    return " ".join([
       # str(row['title']),
       # str(row['abstract']),
        str(row['Summary']),
        str(row['Strengths']),
        str(row['Weaknesses']),
    ])

df_train_original['combined_text'] = df_train_original.apply(combine_text, axis=1)

numerical_cols = ['Soundness', 'Presentation', 'Contribution', 'Confidence']
scaler = StandardScaler()
df_train_original[numerical_cols] = scaler.fit_transform(df_train_original[numerical_cols])

# First, split into 90% train_val and 10% test
train_val_texts, test_texts, train_val_labels, test_labels, train_val_nums, test_nums = train_test_split(
    df_train_original['combined_text'].tolist(),
    df_train_original['label'].tolist(),
    df_train_original[numerical_cols].values,
    test_size=0.1,
    random_state=42,
    stratify=df_train_original['label']
)

# Then, split the 90% into 70% train and 20% validation
train_texts, val_texts, train_labels, val_labels, train_nums, val_nums = train_test_split(
    train_val_texts,
    train_val_labels,
    train_val_nums,
    test_size=2/9,  # 2/9 of 90% = 20%
    random_state=42,
    stratify=train_val_labels
)

MODEL_NAME = 'distilbert-base-cased'

tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME) # Loading the tokenizer

# # Check max number of tokens in combined train, val, test sets
# all_texts = train_texts + val_texts + test_texts

# max_val = 0
# for sent in all_texts:
#     try:
#         sent_tok_len = len(tokenizer.tokenize(sent))
#         if sent_tok_len > max_val:
#             max_val = sent_tok_len
#     except:
#         pass

# print(f"The maximum number of tokens in the dataset is: {max_val}")

"""* With title & abstract --> 6627 tokens
* Without title & abstract --> 6352 tokens
* The maximum amount of tokens in the dataset is 232
I will set the max length and input for the deep learning model to 234
(Max token length + 2 for special characters [CLS] and [SEP])
"""

MAX_LENGTH = 384 #instead of 512

tokenizer = DistilBertTokenizer.from_pretrained(
    MODEL_NAME, #add_special_tokens (?)
    max_length=MAX_LENGTH,
    pad_to_max_length=True,
    truncation=True #truncate because 6352 tokens
    )

# Tokenize function for DistilBERT (no token_type_ids)
def tokenize(sentences, tokenizer):
    input_ids, attention_masks = [], []
    for sentence in tqdm(sentences):
        inputs = tokenizer.encode_plus(
            sentence,
            add_special_tokens=True,
            max_length=MAX_LENGTH,
            padding='max_length',   # replaces deprecated pad_to_max_length
            truncation=True,
            return_attention_mask=True,
            return_token_type_ids=False
        )
        input_ids.append(inputs['input_ids'])
        attention_masks.append(inputs['attention_mask'])

    return np.array(input_ids, dtype='int32'), np.array(attention_masks, dtype='int32')

# Tokenize and unpack input_ids and attention_masks for each split
train_input_ids, train_attention_masks = tokenize(train_texts, tokenizer)
val_input_ids, val_attention_masks = tokenize(val_texts, tokenizer)
test_input_ids, test_attention_masks = tokenize(test_texts, tokenizer)

# Define constants
MODEL_NAME = 'distilbert-base-uncased'
MAX_LENGTH = 384
NUM_CLASSES = 6  # Since you're mapping to 0–5 for 6 rating categories

# Load model config and transformer
config = DistilBertConfig.from_pretrained(MODEL_NAME, output_hidden_states=True, output_attentions=True)
distilbert = TFDistilBertModel.from_pretrained(MODEL_NAME, config=config)

# --- TEXT INPUT BRANCH ---
input_ids_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_token', dtype='int32')
input_masks_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='masked_token', dtype='int32')

#embedding_layer = distilbert(input_ids=input_ids_in, attention_mask=input_masks_in)[0]  # [batch, seq_len, 768]

def distilbert_embed(inputs):
    input_ids, attention_mask = inputs
    return distilbert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state

embedding_layer = tf.keras.layers.Lambda(
    distilbert_embed,
    output_shape=(MAX_LENGTH, 768)  # (sequence_length, hidden_size)
)([input_ids_in, input_masks_in])

X_text = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(embedding_layer)
X_text = tf.keras.layers.GlobalMaxPool1D()(X_text)
X_text = tf.keras.layers.Dense(64, activation='relu')(X_text)

# --- NUMERICAL INPUT BRANCH ---
numerical_in = tf.keras.layers.Input(shape=(4,), name='numerical_scores')  # 4 features
X_num = tf.keras.layers.Dense(16, activation='relu')(numerical_in)

# --- COMBINE ---
X = tf.keras.layers.concatenate([X_text, X_num])
X = tf.keras.layers.Dropout(0.4)(X) #increase dropout
X = tf.keras.layers.Dense(64, activation='relu')(X)
X = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(X)

# --- FINAL MODEL ---
model = tf.keras.Model(inputs=[input_ids_in, input_masks_in, numerical_in], outputs=X)

# Optionally freeze BERT layers
for layer in model.layers[:3]:
    layer.trainable = False

# Compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

import os

# Define output directory name
output_dir = './modelA_outputs'

# Create directory if it doesn't exist
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"✅ Created output directory at: {output_dir}")
else:
    print(f"📁 Output directory already exists at: {output_dir}")

# Checkpoint: Save weights every epoch
model_checkpoint = ModelCheckpoint(
    filepath=os.path.join(output_dir, 'weights.{epoch:02d}.weights.h5'),
    save_weights_only=True,
    verbose=1
)

# Early stopping: Stop if val_loss doesn’t improve for 2 consecutive epochs
early_stopping = EarlyStopping(
    patience=2,
    monitor='val_loss',
    min_delta=0,
    mode='min',
    restore_best_weights=False,
    verbose=1
)

# Learning rate scheduler: Reduce LR on plateau
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=1,
    min_delta=0.01,
    min_lr=1e-6,
    mode='min',
    verbose=1
)

# Combine into a list to pass into model.fit()
callbacks = [model_checkpoint, early_stopping, reduce_lr]

import numpy as np

# Convert all training inputs to np.array
train_input = {
    'input_token': np.array(train_input_ids),
    'masked_token': np.array(train_attention_masks),
    'numerical_scores': np.array(train_nums)
}

val_input = {
    'input_token': np.array(val_input_ids),
    'masked_token': np.array(val_attention_masks),
    'numerical_scores': np.array(val_nums)
}

train_labels = np.array(train_labels)
val_labels = np.array(val_labels)

from tensorflow.keras.optimizers import Adam

"""Conclusion:
Overall performance (accuracy, weighted F1) is better without class weights.

The model still fails to learn anything meaningful for classes 1 and 10 in both cases.

Class weights did not help underrepresented classes and may have hurt more common ones.

Recommendation:
Given this result, class weights are not effective for your current dataset/model setup.

Instead, try this next:
"""

model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=Adam(learning_rate=1e-04),
    metrics=['accuracy']
)
weights = class_weight.compute_class_weight(
     class_weight='balanced',
     classes=np.unique(train_labels),
     y=train_labels
)

class_weight_dict = dict(enumerate(weights))

history = model.fit(
    x=train_input,
    y=train_labels,
    validation_data=(val_input, val_labels),
    epochs=20,
    batch_size=16,
    callbacks=[model_checkpoint, early_stopping, reduce_lr]
)

def plot_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training Accuracy')
    plt.plot(x, val_acc, 'r', label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training Loss')
    plt.plot(x, val_loss, 'r', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.savefig("training_history.pdf", format="pdf", dpi=300)
    plt.show()

    # Print best epochs
    print(f"Lowest Validation Loss: epoch {np.argmin(val_loss)+1}")
    print(f"Highest Validation Accuracy: epoch {np.argmax(val_acc)+1}")

# Call the function after training

plot_history(history)

from sklearn.metrics import accuracy_score, f1_score, classification_report

# --- Step 1: Get best epoch based on validation accuracy ---
def get_max_val_acc_epoch(history):
    return f"{np.argmax(history.history['val_accuracy']) + 1:02d}"  # zero-padded (e.g., 03)

# --- Step 2: Load the best weights ---
best_epoch = get_max_val_acc_epoch(history)
model.load_weights(os.path.join(output_dir, f"weights.{best_epoch}.weights.h5"))
print(f"Loaded weights from best epoch: {best_epoch}")

# --- Step 3: Predict on test set ---
y_test_probs = model.predict({
    'input_token': test_input_ids,
    'masked_token': test_attention_masks,
    'numerical_scores': test_nums
})

y_hat = np.argmax(y_test_probs, axis=1)

# --- Step 4: Evaluation ---
accuracy = accuracy_score(test_labels, y_hat)
f1 = f1_score(test_labels, y_hat, average='weighted')

print(f"Accuracy: {accuracy:.4f}")
print(f"Weighted F1 Score: {f1:.4f}")

# Optional: classification report
labels_6 = ['1', '3', '5', '6', '8', '10']  # if you mapped these from 0–5
print(classification_report(test_labels, y_hat, target_names=labels_6))

# 1) Print out what you’re feeding the model:
for k,v in test_inputs.items():
    print(k, "→", type(v), getattr(v, "shape", None))

# You should see something like:
# input_token       → <class 'numpy.ndarray'>  (n_test, MAX_LEN)
# masked_token      → <class 'numpy.ndarray'>  (n_test, MAX_LEN)
# numerical_scores  → <class 'numpy.ndarray'>  (n_test, 4)

from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np
import os

# --- Step 1: Load best weights for Model A ---
def get_max_val_acc_epoch(history):
    return f"{np.argmax(history.history['val_accuracy']) + 1:02d}"  # zero-padded epoch

best_epoch = get_max_val_acc_epoch(history)
model.load_weights(os.path.join(output_dir, f"weights.{best_epoch}.weights.h5"))
print(f"Loaded Model A weights from best epoch: {best_epoch}")

# --- Step 2: Prepare test input ---
test_input = {
    'input_token': np.array(test_input_ids),
    'masked_token': np.array(test_attention_masks),
    'numerical_scores': np.array(test_nums)
}

# --- Step 3: Predict on test set ---
y_test_probs = model.predict(test_input)
y_pred = np.argmax(y_test_probs, axis=1)

# --- Step 4: Evaluate predictions ---
test_labels = np.array(test_labels)  # make sure it's a NumPy array
accuracy = accuracy_score(test_labels, y_pred)
f1 = f1_score(test_labels, y_pred, average='weighted')

print(f"Test Accuracy (Model A): {accuracy:.4f}")
print(f"Test Weighted F1 Score (Model A): {f1:.4f}")

# Optional: Classification report with mapped labels
label_names = ['1', '3', '5', '6', '8', '10']  # Assuming class indices 0–5
print(classification_report(test_labels, y_pred, target_names=label_names))

from sklearn.model_selection import train_test_split

# Make sure your df is in the exact same order as when you first split
review_ids = df_train_original['ReviewID_Train'].tolist()

# Recreate the same test split
_, test_review_ids = train_test_split(
    review_ids,
    test_size=0.1,
    random_state=42,
    stratify=df_train_original['label']
)

# Map label indices 0–5 → actual rating scores
label_map = {0: 1, 1: 3, 2: 5, 3: 6, 4: 8, 5: 10}
y_pred_scores = [label_map[i] for i in y_pred]

# Create a DataFrame for submission
submission_df = pd.DataFrame({
    'ReviewID': test_review_ids,
    'PredictedRating': y_pred_scores
})

# Save to CSV
submission_df.to_csv('Group10QBUS6850_2025S1_Submission.csv', index=False)
print("Submission file saved as 'Group00QBUS6850_2025S1_Submission.csv'")

def combine_text(row):
    return " ".join([
        str(row['Summary']),
        str(row['Strengths']),
        str(row['Weaknesses'])
    ])

df_test_original['combined_text'] = df_test_original.apply(combine_text, axis=1)
from sklearn.preprocessing import StandardScaler

test_nums = scaler.transform(df_test_original[['Soundness', 'Presentation', 'Contribution', 'Confidence']])

test_input_ids, test_attention_masks = tokenize(df_test_original['combined_text'].tolist(), tokenizer)
test_input = {
    'input_token': np.array(test_input_ids),
    'masked_token': np.array(test_attention_masks),
    'numerical_scores': np.array(test_nums)
}

y_test_probs = model.predict(test_input)
y_pred = np.argmax(y_test_probs, axis=1)
label_map = {0: 1, 1: 3, 2: 5, 3: 6, 4: 8, 5: 10}
y_pred_scores = [label_map[i] for i in y_pred]

submission_df = pd.DataFrame({
    'ReviewID': df_test_original['ReviewID_Test'],
    'PredictedRating': y_pred_scores
})

submission_df.to_csv('Group00QBUS6850_2025S1_Submission_Test.csv', index=False)
print("Submission file saved.")

"""# Model 2 - Summary, Strengths, Weaknesses, Confidence"""

# Combine relevant text columns
#def combine_text(row):
  #  return " ".join([
   #    # str(row['title']),
       # str(row['abstract']),
   #     str(row['Summary']),
   #     str(row['Strengths']),
   #     str(row['Weaknesses']),
   # ])

#df_train_original['combined_text'] = df_train_original.apply(combine_text, axis=1)

numerical_cols2 = ['Confidence']
scaler = StandardScaler()
df_train_original[numerical_cols2] = scaler.fit_transform(df_train_original[numerical_cols2])

# Step 1: Split 90% train+val and 10% test using only 'Confidence'
train_val_texts_2, test_texts_2, train_val_labels_2, test_labels_2, train_val_nums_2, test_nums_2 = train_test_split(
    df_train_original['combined_text'].tolist(),
    df_train_original['label'].tolist(),
    df_train_original[numerical_cols2].values,  # only 'Confidence'
    test_size=0.1,
    random_state=42,
    stratify=df_train_original['label']
)

# Step 2: Split 90% into 70% train and 20% validation
train_texts_2, val_texts_2, train_labels_2, val_labels_2, train_nums_2, val_nums_2 = train_test_split(
    train_val_texts_2,
    train_val_labels_2,
    train_val_nums_2,
    test_size=2/9,  # 2/9 of 90% = 20%
    random_state=42,
    stratify=train_val_labels_2
)

# First, split into 90% train_val and 10% test
train_val_texts, test_texts, train_val_labels, test_labels, train_val_nums, test_nums = train_test_split(
    df_train_original['combined_text'].tolist(),
    df_train_original['label'].tolist(),
    df_train_original[numerical_cols2].values, #only use 'confidence'
    test_size=0.1,
    random_state=42,
    stratify=df_train_original['label']
)

# Then, split the 90% into 70% train and 20% validation
train_texts, val_texts, train_labels, val_labels, train_nums, val_nums = train_test_split(
    train_val_texts,
    train_val_labels,
    train_val_nums,
    test_size=2/9,  # 2/9 of 90% = 20%
    random_state=42,
    stratify=train_val_labels
)

MODEL_NAME = 'distilbert-base-cased'

tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME) # Loading the tokenizer

# # Check max number of tokens in combined train, val, test sets
# all_texts_2 = train_texts_2 + val_texts_2 + test_texts_2

# max_val_2 = 0
# for sent in all_texts_2:
#     try:
#         sent_tok_len = len(tokenizer.tokenize(sent))
#         if sent_tok_len > max_val_2:
#             max_val_2 = sent_tok_len
#     except:
#         pass

# print(f"The maximum number of tokens in the second dataset is: {max_val_2}")

MAX_LENGTH = 384 #instead of 512

tokenizer = DistilBertTokenizer.from_pretrained(
    MODEL_NAME, #add_special_tokens (?)
    max_length=MAX_LENGTH,
    pad_to_max_length=True,
    truncation=True #truncate because 6352 tokens
    )

# Tokenize function for DistilBERT (no token_type_ids)
def tokenize(sentences, tokenizer):
    input_ids, attention_masks = [], []
    for sentence in tqdm(sentences):
        inputs = tokenizer.encode_plus(
            sentence,
            add_special_tokens=True,
            max_length=MAX_LENGTH,
            padding='max_length',   # replaces deprecated pad_to_max_length
            truncation=True,
            return_attention_mask=True,
            return_token_type_ids=False
        )
        input_ids.append(inputs['input_ids'])
        attention_masks.append(inputs['attention_mask'])

    return np.array(input_ids, dtype='int32'), np.array(attention_masks, dtype='int32')

# Tokenize and unpack input_ids and attention_masks for each split - Model 2
train_input_ids_2, train_attention_masks_2 = tokenize(train_texts_2, tokenizer)
val_input_ids_2, val_attention_masks_2 = tokenize(val_texts_2, tokenizer)
test_input_ids_2, test_attention_masks_2 = tokenize(test_texts_2, tokenizer)

# Define constants
MODEL_NAME = 'distilbert-base-uncased'
MAX_LENGTH = 384
NUM_CLASSES = 6  # Ratings mapped to labels 0–5

# Load model config and transformer
config = DistilBertConfig.from_pretrained(MODEL_NAME, output_hidden_states=True, output_attentions=True)
distilbert = TFDistilBertModel.from_pretrained(MODEL_NAME, config=config)

# --- TEXT INPUT BRANCH ---
input_ids_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_token', dtype='int32')
input_masks_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='masked_token', dtype='int32')

def distilbert_embed(inputs):
    input_ids, attention_mask = inputs
    return distilbert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state

embedding_layer = tf.keras.layers.Lambda(
    distilbert_embed,
    output_shape=(MAX_LENGTH, 768)
)([input_ids_in, input_masks_in])

X_text = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(embedding_layer)
X_text = tf.keras.layers.GlobalMaxPool1D()(X_text)
X_text = tf.keras.layers.Dense(64, activation='relu')(X_text)

# --- NUMERICAL INPUT BRANCH ---
numerical_in = tf.keras.layers.Input(shape=(1,), name='numerical_scores')  # Only Confidence now
X_num = tf.keras.layers.Dense(16, activation='relu')(numerical_in)

# --- COMBINE ---
X = tf.keras.layers.concatenate([X_text, X_num])
X = tf.keras.layers.Dropout(0.4)(X)
X = tf.keras.layers.Dense(64, activation='relu')(X)
X = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(X)

# --- FINAL MODEL ---
model = tf.keras.Model(inputs=[input_ids_in, input_masks_in, numerical_in], outputs=X)

# Optionally freeze BERT layers
for layer in model.layers[:3]:
    layer.trainable = False

# Compilehttps://github.com/dblilienthal/Multiclass-Text-Classification-with-DistilBERT-on-COVID-19-Tweetshttps://github.com/dblilienthal/Multiclass-Text-Classification-with-DistilBERT-on-COVID-19-Tweets
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

import os

# Define output directory name
output_dir = './modelB_outputs'

# Create directory if it doesn't exist
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"Created output directory at: {output_dir}")
else:
    print(f"Output directory already exists at: {output_dir}")

# Checkpoint: Save weights every epoch
model_checkpoint = ModelCheckpoint(
    filepath=os.path.join(output_dir, 'weights.{epoch:02d}.weights.h5'),
    save_weights_only=True,
    verbose=1
)

# Early stopping: Stop if val_loss doesn’t improve for 3 consecutive epochs
early_stopping = EarlyStopping(
    patience=2,
    monitor='val_loss',
    min_delta=0,
    mode='min',
    restore_best_weights=False,
    verbose=1
)

# Learning rate scheduler: Reduce LR on plateau
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=1,
    min_delta=0.01,
    min_lr=1e-6,
    mode='min',
    verbose=1
)

# Combine into a list to pass into model.fit()
callbacks = [model_checkpoint, early_stopping, reduce_lr]

# Convert all Model B (_2) training inputs to np.array
train_input_2 = {
    'input_token': np.array(train_input_ids_2),
    'masked_token': np.array(train_attention_masks_2),
    'numerical_scores': np.array(train_nums_2)
}

val_input_2 = {
    'input_token': np.array(val_input_ids_2),
    'masked_token': np.array(val_attention_masks_2),
    'numerical_scores': np.array(val_nums_2)
}

train_labels_2 = np.array(train_labels_2)
val_labels_2 = np.array(val_labels_2)

model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=Adam(learning_rate=1e-04),
    metrics=['accuracy']
)
weights = class_weight.compute_class_weight(
     class_weight='balanced',
     classes=np.unique(train_labels),
     y=train_labels
)

class_weight_dict = dict(enumerate(weights))

# Train model on _2 data
history_2 = model.fit(
    x=train_input_2,
    y=train_labels_2,
    validation_data=(val_input_2, val_labels_2),
    epochs=10,
    batch_size=16,
    callbacks=[model_checkpoint, early_stopping, reduce_lr]
)

def plot_history_2(history_2):
    acc = history_2.history['accuracy']
    val_acc = history_2.history['val_accuracy']
    loss = history_2.history['loss']
    val_loss = history_2.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))

    # Accuracy plot
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training Accuracy')
    plt.plot(x, val_acc, 'r', label='Validation Accuracy')
    plt.title('Model B: Training and Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss plot
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training Loss')
    plt.plot(x, val_loss, 'r', label='Validation Loss')
    plt.title('Model B: Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Best epoch info
    print(f"Lowest Validation Loss: epoch {np.argmin(val_loss)+1}")
    print(f"Highest Validation Accuracy: epoch {np.argmax(val_acc)+1}")

# Call it
plot_history_2(history_2)

from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np
import os

# --- Step 1: Get best epoch based on validation accuracy ---
def get_max_val_acc_epoch(history_2):
    return f"{np.argmax(history_2.history['val_accuracy']) + 1:02d}"  # zero-padded (e.g., 03)

# --- Step 2: Load the best weights from modelB_outputs ---
best_epoch_2 = get_max_val_acc_epoch(history_2)
model.load_weights(os.path.join(output_dir, f"weights.{best_epoch_2}.weights.h5"))
print(f" Loaded weights from best epoch: {best_epoch_2}")

# --- Step 3: Predict on test set ---
test_input_2 = {
    'input_token': np.array(test_input_ids_2),
    'masked_token': np.array(test_attention_masks_2),
    'numerical_scores': np.array(test_nums_2)
}

y_test_probs_2 = model.predict(test_input_2)
y_hat_2 = np.argmax(y_test_probs_2, axis=1)

# --- Step 4: Evaluation ---
test_labels_2 = np.array(test_labels_2)  # ensure it's a NumPy array
accuracy_2 = accuracy_score(test_labels_2, y_hat_2)
f1_2 = f1_score(test_labels_2, y_hat_2, average='weighted')

print(f"Accuracy (Model B): {accuracy_2:.4f}")
print(f" Weighted F1 Score (Model B): {f1_2:.4f}")

# Optional: classification report
labels_6 = ['1', '3', '5', '6', '8', '10']  # mapped from class labels 0–5
print(classification_report(test_labels_2, y_hat_2, target_names=labels_6))

f1_macro_2 = f1_score(test_labels_2, y_hat_2, average='macro')
print(f"Macro F1 Score (Model B): {f1_macro_2:.4f}")

# -*- coding: utf-8 -*-
"""TASK A - EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M9t2fpUMGWgBe6GpKfELIFe2TEX9ifaB
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

rating_df = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Rating_Train.csv')

submissions_df = pd.read_csv('/content/drive/MyDrive/ICLR2025 Files/ICLR2025_Submissions.csv')

submissions_df.head()

rating_df.head()

"""# Data Preprocessing"""

from sklearn.preprocessing import MinMaxScaler

num_cols = rating_df.select_dtypes(include=['number']).columns.tolist()
num_cols = [c for c in num_cols if c not in ['ReviewID_Train','Rating']]

scaler = MinMaxScaler()
rating_df[num_cols] = scaler.fit_transform(rating_df[num_cols])

rating_df.head()

import matplotlib.pyplot as plt

# 1. Extract numeric columns, excluding the ID
num_cols = [
    c for c in rating_df.select_dtypes(include='number').columns
    if c not in ['ReviewID_Train']
]

# 2. Compute correlation matrix
corr = rating_df[num_cols].corr()

# 3. Plot heatmap
fig, ax = plt.subplots(figsize=(10, 8))
cax = ax.imshow(corr, interpolation='nearest', aspect='auto')
fig.colorbar(cax, ax=ax)

# 4. Tick labels
ax.set_xticks(range(len(num_cols)))
ax.set_yticks(range(len(num_cols)))
ax.set_xticklabels(num_cols, rotation=90)
ax.set_yticklabels(num_cols)

ax.set_title('Correlation Heatmap of Numerical Features')
plt.tight_layout()
plt.show()

"""# Handling Missing Values"""

# 1.1 Shapes
print("Train set:", rating_df.shape)

# 1.2 Column types & non-null counts
print("\n--- TRAIN INFO ---")
rating_df.info()

# 2 Count of nulls per column
print("Rating Dataset Missing Values:")
print(rating_df.isnull().sum().sort_values(ascending=False))

# 1.1 Shapes
print("Submission Dataset:", submissions_df.shape)

# 1.2 Column types & non-null counts
print("\n--- TRAIN INFO ---")
submissions_df.info()

# 2 Count of nulls per column
print("Submission Dataset Missing Values:")
print(submissions_df.isnull().sum().sort_values(ascending=False))

# 1) How many completely identical rows?
print("Total duplicate rows:", submissions_df.duplicated().sum())

# 2) How many duplicate PaperIDs?
print("Duplicate PaperID count:", submissions_df.duplicated(subset="PaperID").sum())

# 1) How many completely identical rows?
print("Total duplicate rows in rating dataset:", rating_df.duplicated().sum())

# 2) How many duplicate PaperIDs?
print("Duplicate Review ID count in rating dataset:", rating_df.duplicated(subset="ReviewID_Train").sum())

# Check basic information about the datasets (e.g., data types, number of non-null entries)
print("\nSubmissions data info:")
print(submissions_df.info())

print("\nRating data info:")
print(rating_df.info())

# Drop TLDR column
submissions_df.drop(columns=['tldr'], inplace=True)

# 1. Fill NaNs in the three text columns
rating_df[['Summary','Strengths','Weaknesses']] = (
    rating_df[['Summary','Strengths','Weaknesses']]
    .fillna('Not Provided')
)

# 2. Verify
print(rating_df[['Summary','Strengths','Weaknesses']].isnull().sum())

"""# Rating Distribution Visualisation"""

import matplotlib.pyplot as plt

counts = rating_df['Rating'].value_counts().sort_index()

plt.figure(figsize=(8, 5))
plt.bar(counts.index, counts.values)
plt.xticks(counts.index)
plt.xlabel('Rating')
plt.ylabel('Count')
plt.title('Distribution of Review Ratings')
plt.tight_layout()
plt.show()

counts = rating_df['Rating'].value_counts().sort_index()
props  = counts / counts.sum() * 100

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

ax1.bar(counts.index, counts.values)
ax1.set_title('Count Distribution of Review Ratings')
ax1.set_xlabel('Rating')
ax1.set_ylabel('Count')
ax1.set_xticks(counts.index)

ax2.bar(props.index, props.values)
ax2.set_title('Percentage Distribution of Review Ratings')
ax2.set_xlabel('Rating')
ax2.set_ylabel('Percentage (%)')
ax2.set_xticks(props.index)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# 1. Set a clean white‐grid style via Seaborn
sns.set_style('whitegrid')

# 2. Prepare the data
counts = rating_df['Rating'].value_counts().sort_index()
ratings = counts.index.values
freqs   = counts.values

# 3. Compute bubble sizes and colors
max_bubble = 3000
sizes = (freqs / freqs.max()) ** 1.5 * max_bubble   # emphasize larger bubbles
colors = freqs                                      # map frequency → color

# 4. Create the plot
fig, ax = plt.subplots(figsize=(10, 6))
scatter = ax.scatter(
    ratings,
    freqs,
    s=sizes,
    c=colors,
    cmap='viridis',
    alpha=0.75,
    edgecolors='k',
    linewidth=0.5
)

# 5. Add a colorbar
cbar = fig.colorbar(scatter, ax=ax, pad=0.02)
cbar.set_label('Count', fontsize=12)

# 6. Annotate each bubble with its count
for x, y in zip(ratings, freqs):
    ax.annotate(
        f"{y}",
        xy=(x, y),
        xytext=(0, 5),
        textcoords='offset points',
        ha='center',
        va='bottom',
        fontsize=10,
        color='black'
    )

# 7. Labels, title, and ticks
ax.set_title('Review Rating Frequencies', fontsize=16, pad=20)
ax.set_xlabel('Rating', fontsize=14)
ax.set_ylabel('Number of Reviews', fontsize=14)
ax.set_xticks(ratings)
ax.set_ylim(0, freqs.max() * 1.1)

plt.tight_layout()
plt.show()

"""# Text Length Analysis

Text Length Analysis for Title and Abstract on the Submissions Dataset

Text Length Analysis for Title
"""

def compute_text_lengths(rating_df):
    min_length = float('inf')
    max_length = 0
    total_length = 0
    total_titles = 0

    #Loop
    for title in submissions_df['title']:
        title_length = len(title.split())

        #Upsdate max length
        min_length = min(min_length, title_length)
        max_length = max(max_length, title_length)

        total_length += title_length
        total_titles += 1

    #Avg length
    avg_length = total_length / total_titles if total_titles > 0 else 0

    return min_length, max_length, avg_length

min_length, max_length, avg_length = compute_text_lengths(submissions_df)
print(f"Minimum title length: {min_length} words")
print(f"Maximum title length: {max_length} words")
print(f"Average title length: {avg_length:.2f} words")

def min_words(submissions_df):
    min_words = 0
    for title in submissions_df['title']:
        words = title.split()
        min_words = min(min_words, len(words))
    return min_words

min_title_words = min_words(submissions_df)
print(f"The minimum number of words in the title is: {min_title_words}")

"""Text Length Analysis for Abstract"""

def compute_text_lengths(submissions_df):
    min_length_abstract = float('inf')  # Initialize with infinity for min comparison
    max_length_abstract = 0
    total_length_abstract = 0
    total_abstract = 0

    # Loop through the abstracts
    for abstract in submissions_df['abstract']:
        abstract_length = len(abstract.split())  # Count the number of words in each abstract

        # Update the min and max length
        min_length_abstract = min(min_length_abstract, abstract_length)
        max_length_abstract = max(max_length_abstract, abstract_length)

        total_length_abstract += abstract_length
        total_abstract += 1

    # Calculate average length
    avg_length_abstract = total_length_abstract / total_abstract if total_abstract > 0 else 0

    return min_length_abstract, max_length_abstract, avg_length_abstract

# Call the function and print the results for abstract lengths
min_length_abstract, max_length_abstract, avg_length_abstract = compute_text_lengths(submissions_df)
print(f"Minimum abstract length: {min_length_abstract} words")
print(f"Maximum abstract length: {max_length_abstract} words")
print(f"Average abstract length: {avg_length_abstract:.2f} words")

from textblob import TextBlob
import matplotlib.pyplot as plt

# 1. Compute word‐count (verbosity) features
submissions_df['len_title']    = submissions_df['title']  .fillna('').str.split().apply(len)
submissions_df['len_abstract'] = submissions_df['abstract'].fillna('').str.split().apply(len)

# 2. Compute sentiment polarity features
submissions_df['sent_title']    = submissions_df['title']  .fillna('').apply(lambda t: TextBlob(t).sentiment.polarity)
submissions_df['sent_abstract'] = submissions_df['abstract'].fillna('').apply(lambda t: TextBlob(t).sentiment.polarity)

# 3. Correlation heatmap of the four new features
features = ['len_title','len_abstract','sent_title','sent_abstract']
corr = submissions_df[features].corr()

fig, ax = plt.subplots(figsize=(6, 5))
cax = ax.imshow(corr, interpolation='nearest', aspect='auto')
fig.colorbar(cax, ax=ax)

ax.set_xticks(range(len(features)))
ax.set_yticks(range(len(features)))
ax.set_xticklabels(features, rotation=90)
ax.set_yticklabels(features)

ax.set_title('Correlation Heatmap: Title & Abstract Features')
plt.tight_layout()
plt.show()

"""Text Length Analysis on the Rating Dataset"""

import pandas as pd
import matplotlib.pyplot as plt

# Safely compute word counts by converting all entries to strings
title_length = rating_df['title'].apply(lambda x: len(str(x).split()))
abstract_length = rating_df['abstract'].apply(lambda x: len(str(x).split()))
summary_length = rating_df['Summary'].apply(lambda x: len(str(x).split()))
strengths_length = rating_df['Strengths'].apply(lambda x: len(str(x).split()))
weaknesses_length = rating_df['Weaknesses'].apply(lambda x: len(str(x).split()))

# Create a summary statistics DataFrame
stats_df = pd.DataFrame({
    'Title': [title_length.min(), title_length.max(), title_length.mean(), title_length.median()],
    'Abstract': [abstract_length.min(), abstract_length.max(), abstract_length.mean(), abstract_length.median()],
    'Summary': [summary_length.min(), summary_length.max(), summary_length.mean(), summary_length.median()],
    'Strengths': [strengths_length.min(), strengths_length.max(), strengths_length.mean(), strengths_length.median()],
    'Weaknesses': [weaknesses_length.min(), weaknesses_length.max(), weaknesses_length.mean(), weaknesses_length.median()],
}, index=['Min', 'Max', 'Mean', 'Median'])

# Display the statistics table using matplotlib
fig, ax = plt.subplots(figsize=(10, 3))
ax.axis('tight')
ax.axis('off')

table = ax.table(cellText=stats_df.round(2).values,
                 colLabels=stats_df.columns,
                 rowLabels=stats_df.index,
                 cellLoc='center',
                 loc='center')
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)

plt.title('Word Count Statistics per Field', fontsize=14, pad=20)
plt.tight_layout()
plt.show()

# 1. Token counts
rating_df['len_summary']    = rating_df['Summary'].str.split().apply(len)
rating_df['len_strengths']  = rating_df['Strengths'].str.split().apply(len)
rating_df['len_weaknesses'] = rating_df['Weaknesses'].str.split().apply(len)

# 2. Pros–cons balance ratio
rating_df['str_wkn_ratio']  = rating_df['len_strengths'] / (rating_df['len_weaknesses'] + 1)

from textblob import TextBlob

rating_df['sent_summary']    = rating_df['Summary']   .fillna('')\
                                    .apply(lambda t: TextBlob(t).sentiment.polarity)
rating_df['sent_strengths']  = rating_df['Strengths'] .fillna('')\
                                    .apply(lambda t: TextBlob(t).sentiment.polarity)
rating_df['sent_weaknesses'] = rating_df['Weaknesses'].fillna('')\
                                    .apply(lambda t: TextBlob(t).sentiment.polarity)

rating_df.head()

import matplotlib.pyplot as plt

# 1. Select numeric columns (excluding any ID column)
num_cols = [
    c for c in rating_df.select_dtypes(include='number').columns
    if c not in ['ReviewID_Train']
]

# 2. Compute the correlation matrix
corr = rating_df[num_cols].corr()

# 3. Plot the heatmap
fig, ax = plt.subplots(figsize=(10, 8))
cax = ax.imshow(corr, interpolation='nearest', aspect='auto')
fig.colorbar(cax, ax=ax)

# 4. Configure tick labels
ax.set_xticks(range(len(num_cols)))
ax.set_yticks(range(len(num_cols)))
ax.set_xticklabels(num_cols, rotation=90)
ax.set_yticklabels(num_cols)

# 5. Title and layout
ax.set_title('Feature Correlation Heatmap')
plt.tight_layout()
plt.show()

"""# Keyword Frequency Analysis"""

from collections import Counter
import ast
import matplotlib.pyplot as plt
import pandas as pd

# 1. Parse the 'keywords' column into Python lists
submissions_df['parsed_keywords'] = submissions_df['keywords'].apply(
    lambda x: ast.literal_eval(x) if pd.notnull(x) else []
)

# 2. Flatten and normalize all keywords
all_keywords = [
    kw.strip().lower()
    for kws in submissions_df['parsed_keywords']
    for kw in kws
]

# 3. Count keyword frequencies
keyword_counts = Counter(all_keywords)

# 4. Build a DataFrame of keyword counts, sorted descending
keyword_df = (
    pd.DataFrame(keyword_counts.items(), columns=['Keyword', 'Count'])
      .sort_values(by='Count', ascending=False)
)

# 5. Plot the top 20 keywords
plt.figure(figsize=(12, 6))
plt.barh(
    keyword_df['Keyword'].head(20)[::-1],
    keyword_df['Count'].head(20)[::-1]
)
plt.xlabel("Frequency")
plt.ylabel("Keyword")
plt.title("Top 20 Most Frequent Keywords in ICLR Submissions")
plt.tight_layout()
plt.show()

"""# Defining Machine Learning Model

# 1. Regression Model
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Select numeric features
features = ['Soundness', 'Presentation', 'Contribution', 'Confidence']
X = rating_df[features]
y = rating_df['Rating']

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model
reg = LinearRegression()
reg.fit(X_train, y_train)

# Predict
y_pred = reg.predict(X_test)

# Evaluate
rmse = mean_squared_error(y_test, y_pred)
print(f'Regression RMSE: {rmse:.4f}')

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Create classes: low=1-3, medium=4-6, high=7-9 (adjust bins as needed)
rating_df['RatingClass'] = pd.cut(rating_df['Rating'], bins=[0,3,6,10], labels=[0,1,2])

features = ['Soundness', 'Presentation', 'Contribution', 'Confidence']
X = rating_df[features]
y = rating_df['RatingClass']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

texts = rating_df['abstract'].fillna('')

vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
X_text = vectorizer.fit_transform(texts)

kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(X_text)

rating_df['Cluster'] = clusters

print(rating_df[['abstract', 'Cluster']].head())
